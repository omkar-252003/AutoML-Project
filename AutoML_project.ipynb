{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMthBtGTypmB"
      },
      "source": [
        "Auto ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4NiwvMokmLq",
        "outputId": "e20daf72-eb89-4621-81d7-b40f6014273c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tpot\n",
            "  Downloading TPOT-0.12.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting h2o\n",
            "  Downloading h2o-3.46.0.5.tar.gz (265.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.6/265.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from tpot) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from tpot) (1.13.1)\n",
            "Collecting deap>=1.2 (from tpot)\n",
            "  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting update-checker>=0.16 (from tpot)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.10/dist-packages (from tpot) (4.66.5)\n",
            "Collecting stopit>=1.1.1 (from tpot)\n",
            "  Downloading stopit-1.1.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from tpot) (1.4.2)\n",
            "Requirement already satisfied: xgboost>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tpot) (2.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from h2o) (2.32.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from h2o) (0.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2024.8.30)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost>=1.1.0->tpot) (2.23.4)\n",
            "Downloading TPOT-0.12.2-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.4/87.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Building wheels for collected packages: h2o, stopit\n",
            "  Building wheel for h2o (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for h2o: filename=h2o-3.46.0.5-py2.py3-none-any.whl size=265646558 sha256=75a226f1cbb8ca3eb4652c1ca35e1bb72b2b5f3ca62fd0eb55d3821b22e926ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/46/4f/9b366522399306d7849672d58aefb44c9b73378d710bde2853\n",
            "  Building wheel for stopit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stopit: filename=stopit-1.1.2-py3-none-any.whl size=11938 sha256=39b3508c571d70e70878e85d8093b31f79bfe618bdc65bb4a2e0e789a2b52075\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/f9/87/bf5b3d565c2a007b4dae9d8142dccc85a9f164e517062dd519\n",
            "Successfully built h2o stopit\n",
            "Installing collected packages: stopit, deap, update-checker, h2o, tpot\n",
            "Successfully installed deap-1.4.1 h2o-3.46.0.5 stopit-1.1.2 tpot-0.12.2 update-checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "pip install tpot h2o pandas scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBjOlZf-z6A4"
      },
      "source": [
        "Loading The dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8SNzGwgktyz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset (you can replace the URL with the path to your dataset)\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\")\n",
        "\n",
        "# Preprocess the dataset (handle missing values, encode categorical data)\n",
        "data = data.dropna()  # Simple preprocessing, more can be added\n",
        "X = data.drop(\"median_house_value\", axis=1)\n",
        "y = data[\"median_house_value\"]\n",
        "\n",
        "# Encode categorical variables\n",
        "X = pd.get_dummies(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBGGd1XTz-fg"
      },
      "source": [
        "H2O.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "V1vBwnwKyi2k",
        "outputId": "11780847-fadd-4bc6-d501-451af21ba8c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
            "Attempting to start a local H2O server...\n",
            "  Java Version: openjdk version \"11.0.24\" 2024-07-16; OpenJDK Runtime Environment (build 11.0.24+8-post-Ubuntu-1ubuntu322.04); OpenJDK 64-Bit Server VM (build 11.0.24+8-post-Ubuntu-1ubuntu322.04, mixed mode, sharing)\n",
            "  Starting server from /usr/local/lib/python3.10/dist-packages/h2o/backend/bin/h2o.jar\n",
            "  Ice root: /tmp/tmp2z2qvbaq\n",
            "  JVM stdout: /tmp/tmp2z2qvbaq/h2o_unknownUser_started_from_python.out\n",
            "  JVM stderr: /tmp/tmp2z2qvbaq/h2o_unknownUser_started_from_python.err\n",
            "  Server is running at http://127.0.0.1:54321\n",
            "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-2.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-2 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-2 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-2 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-2 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-2 .h2o-table th,\n",
              "#h2o-table-2 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-2 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-2\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption></caption>\n",
              "    <thead></thead>\n",
              "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>07 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.46.0.5</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>1 month and 16 days</td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_dcqzyg</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>3.170 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>locked, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://127.0.0.1:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.10.12 final</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ],
            "text/plain": [
              "--------------------------  -----------------------------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         07 secs\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.46.0.5\n",
              "H2O_cluster_version_age:    1 month and 16 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_dcqzyg\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    3.170 Gb\n",
              "H2O_cluster_total_cores:    2\n",
              "H2O_cluster_allowed_cores:  2\n",
              "H2O_cluster_status:         locked, healthy\n",
              "H2O_connection_url:         http://127.0.0.1:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}\n",
              "H2O_internal_security:      False\n",
              "Python_version:             3.10.12 final\n",
              "--------------------------  -----------------------------------------------------------------------------------------"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "AutoML progress: |███████████████████████████████████████████████████████████████| (done) 100%\n",
            "Best Model Performance:\n",
            "RMSE: 45614.5622336612\n",
            "R²: 0.8478493030314267\n",
            "\n",
            "AutoML Leaderboard:\n",
            "                                             model_id           rmse  \\\n",
            "0   StackedEnsemble_AllModels_1_AutoML_1_20241016_...   45518.277077   \n",
            "1   StackedEnsemble_BestOfFamily_1_AutoML_1_202410...   46047.067123   \n",
            "2                      GBM_4_AutoML_1_20241016_100602   46385.181054   \n",
            "3                      GBM_3_AutoML_1_20241016_100602   46581.911480   \n",
            "4                      GBM_1_AutoML_1_20241016_100602   46946.258931   \n",
            "5                      GBM_2_AutoML_1_20241016_100602   47041.056760   \n",
            "6         GBM_grid_1_AutoML_1_20241016_100602_model_2   47463.498986   \n",
            "7                      GBM_5_AutoML_1_20241016_100602   47892.618126   \n",
            "8         GBM_grid_1_AutoML_1_20241016_100602_model_1   48653.359704   \n",
            "9                  XGBoost_3_AutoML_1_20241016_100602   49440.374585   \n",
            "10                     DRF_1_AutoML_1_20241016_100602   49452.745647   \n",
            "11    XGBoost_grid_1_AutoML_1_20241016_100602_model_3   49898.976036   \n",
            "12    XGBoost_grid_1_AutoML_1_20241016_100602_model_2   50181.728349   \n",
            "13                     XRT_1_AutoML_1_20241016_100602   50206.926868   \n",
            "14                 XGBoost_1_AutoML_1_20241016_100602   50377.112379   \n",
            "15                 XGBoost_2_AutoML_1_20241016_100602   50510.761190   \n",
            "16    XGBoost_grid_1_AutoML_1_20241016_100602_model_1   50612.301574   \n",
            "17  DeepLearning_grid_3_AutoML_1_20241016_100602_m...   54122.699907   \n",
            "18  DeepLearning_grid_2_AutoML_1_20241016_100602_m...   54975.203989   \n",
            "19            DeepLearning_1_AutoML_1_20241016_100602   57847.886995   \n",
            "20  DeepLearning_grid_1_AutoML_1_20241016_100602_m...   58863.235232   \n",
            "21                     GLM_1_AutoML_1_20241016_100602  115052.287701   \n",
            "\n",
            "             mse           mae     rmsle  mean_residual_deviance  \n",
            "0   2.071914e+09  29610.273565  0.223234            2.071914e+09  \n",
            "1   2.120332e+09  30058.051087  0.226333            2.120332e+09  \n",
            "2   2.151585e+09  30259.423155  0.228189            2.151585e+09  \n",
            "3   2.169874e+09  30729.998094  0.230647            2.169874e+09  \n",
            "4   2.203951e+09  31255.127468  0.232963            2.203951e+09  \n",
            "5   2.212861e+09  31012.334732  0.232230            2.212861e+09  \n",
            "6   2.252784e+09  30985.107439  0.234168            2.252784e+09  \n",
            "7   2.293703e+09  31549.641071  0.237325            2.293703e+09  \n",
            "8   2.367149e+09  32126.717482  0.241992            2.367149e+09  \n",
            "9   2.444351e+09  33200.739515       NaN            2.444351e+09  \n",
            "10  2.445574e+09  32754.256244  0.239512            2.445574e+09  \n",
            "11  2.489908e+09  32443.755414       NaN            2.489908e+09  \n",
            "12  2.518206e+09  33053.106535  0.247583            2.518206e+09  \n",
            "13  2.520736e+09  33542.963192  0.244259            2.520736e+09  \n",
            "14  2.537853e+09  33845.259786       NaN            2.537853e+09  \n",
            "15  2.551337e+09  33733.953259  0.253702            2.551337e+09  \n",
            "16  2.561605e+09  33027.807799  0.245317            2.561605e+09  \n",
            "17  2.929267e+09  37143.527871       NaN            2.929267e+09  \n",
            "18  3.022273e+09  37893.889925       NaN            3.022273e+09  \n",
            "19  3.346378e+09  39873.211813  0.286454            3.346378e+09  \n",
            "20  3.464880e+09  40895.722327       NaN            3.464880e+09  \n",
            "21  1.323703e+10  90733.124661  0.587630            1.323703e+10  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/h2o/frame.py:1981: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
            "\n",
            "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "H2O session _sid_8bcd closed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-05e0ec8dadc0>:48: H2ODeprecationWarning: Deprecated, use ``h2o.cluster().shutdown()``.\n",
            "  h2o.shutdown()\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "\n",
        "# Step 1: Load and Preprocess the Dataset\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\")\n",
        "data = data.dropna()\n",
        "X = data.drop(\"median_house_value\", axis=1)\n",
        "y = data[\"median_house_value\"]\n",
        "\n",
        "# Convert categorical data using one-hot encoding\n",
        "X = pd.get_dummies(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Initialize H2O and Convert Data\n",
        "h2o.init()\n",
        "\n",
        "# Convert data to H2O frames\n",
        "train = h2o.H2OFrame(pd.concat([X_train, y_train], axis=1))\n",
        "test = h2o.H2OFrame(pd.concat([X_test, y_test], axis=1))\n",
        "\n",
        "# Define response and predictor columns\n",
        "response = \"median_house_value\"\n",
        "predictors = [col for col in train.columns if col != response]\n",
        "\n",
        "# Step 3: Run H2O AutoML\n",
        "aml = H2OAutoML(max_models=20, seed=42)\n",
        "aml.train(x=predictors, y=response, training_frame=train)\n",
        "\n",
        "# Step 4: Get the Best Model and Evaluate Performance\n",
        "best_model = aml.leader\n",
        "performance = best_model.model_performance(test)\n",
        "\n",
        "print(\"Best Model Performance:\")\n",
        "print(f\"RMSE: {performance.rmse()}\")\n",
        "print(f\"R²: {performance.r2()}\")\n",
        "\n",
        "# Step 5: Compare Other Models from AutoML Leaderboard\n",
        "leaderboard = aml.leaderboard.as_data_frame()\n",
        "print(\"\\nAutoML Leaderboard:\")\n",
        "print(leaderboard)\n",
        "\n",
        "# Step 6: Save the Best Model\n",
        "h2o.save_model(best_model, path=\"best_model_h2o\", force=True)\n",
        "\n",
        "# Shutdown H2O\n",
        "h2o.shutdown()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "id": "JYSC3DyOlI-g",
        "outputId": "a89327c8-a164-440d-df15-aa4f05a7aea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
            "Attempting to start a local H2O server...\n",
            "  Java Version: openjdk version \"11.0.24\" 2024-07-16; OpenJDK Runtime Environment (build 11.0.24+8-post-Ubuntu-1ubuntu322.04); OpenJDK 64-Bit Server VM (build 11.0.24+8-post-Ubuntu-1ubuntu322.04, mixed mode, sharing)\n",
            "  Starting server from /usr/local/lib/python3.10/dist-packages/h2o/backend/bin/h2o.jar\n",
            "  Ice root: /tmp/tmpjqro8_43\n",
            "  JVM stdout: /tmp/tmpjqro8_43/h2o_unknownUser_started_from_python.out\n",
            "  JVM stderr: /tmp/tmpjqro8_43/h2o_unknownUser_started_from_python.err\n",
            "  Server is running at http://127.0.0.1:54321\n",
            "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-1.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-1 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-1 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-1 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-1 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-1 .h2o-table th,\n",
              "#h2o-table-1 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption></caption>\n",
              "    <thead></thead>\n",
              "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>06 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.46.0.5</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>1 month and 16 days</td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_657pji</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>3.170 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>locked, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://127.0.0.1:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.10.12 final</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ],
            "text/plain": [
              "--------------------------  -----------------------------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         06 secs\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.46.0.5\n",
              "H2O_cluster_version_age:    1 month and 16 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_657pji\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    3.170 Gb\n",
              "H2O_cluster_total_cores:    2\n",
              "H2O_cluster_allowed_cores:  2\n",
              "H2O_cluster_status:         locked, healthy\n",
              "H2O_connection_url:         http://127.0.0.1:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}\n",
              "H2O_internal_security:      False\n",
              "Python_version:             3.10.12 final\n",
              "--------------------------  -----------------------------------------------------------------------------------------"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "AutoML progress: |███████████████████████████████████████████████████████████████| (done) 100%\n",
            "ModelMetricsRegressionGLM: stackedensemble\n",
            "** Reported on test data. **\n",
            "\n",
            "MSE: 2074112182.0582428\n",
            "RMSE: 45542.42178516908\n",
            "MAE: 29544.644468483875\n",
            "RMSLE: 0.2209084201894307\n",
            "Mean Residual Deviance: 2074112182.0582428\n",
            "R^2: 0.8483301819180165\n",
            "Null degrees of freedom: 4086\n",
            "Residual degrees of freedom: 4074\n",
            "Null deviance: 55895408761548.32\n",
            "Residual deviance: 8476896488072.038\n",
            "AIC: 99303.9932264633\n",
            "H2O session _sid_8fc5 closed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-bc22dc63bdb9>:28: H2ODeprecationWarning: Deprecated, use ``h2o.cluster().shutdown()``.\n",
            "  h2o.shutdown()\n"
          ]
        }
      ],
      "source": [
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "\n",
        "# Initialize H2O server\n",
        "h2o.init()\n",
        "\n",
        "# Convert data to H2O frame\n",
        "train = h2o.H2OFrame(pd.concat([X_train, y_train], axis=1))\n",
        "test = h2o.H2OFrame(pd.concat([X_test, y_test], axis=1))\n",
        "\n",
        "# Define the response and predictor columns\n",
        "response = \"median_house_value\"\n",
        "predictors = [col for col in train.columns if col != response]\n",
        "\n",
        "# Run AutoML\n",
        "aml = H2OAutoML(max_models=20, seed=42)\n",
        "aml.train(x=predictors, y=response, training_frame=train)\n",
        "\n",
        "# Get the best model and evaluate\n",
        "best_model = aml.leader\n",
        "performance = best_model.model_performance(test)\n",
        "print(performance)\n",
        "\n",
        "# Save the model for future use\n",
        "h2o.save_model(best_model, path=\"best_model_h2o\", force=True)\n",
        "\n",
        "# Shutdown H2O\n",
        "h2o.shutdown()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_ImsvD-zk-E"
      },
      "source": [
        "TPOT - Tree-based Pipeline Optimization Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367,
          "referenced_widgets": [
            "e469ca1328ab4199a6703ca6bce76310",
            "3f030dfc4ef945debee74388acf789b4",
            "fe4da3b043e4467d82d06ba0d994e9ba",
            "5158cd731c5647c89f2005724128e926",
            "37c1e46173e24f7696e0eb6d2e018b4d",
            "e6a1a6bc753d4d89a40b5852435d8347",
            "f11e0809417d4f07bce601b0b3c32087",
            "bcfbce17c4c945f0a736edba9fa1e553",
            "7519b1a6f60a433db213314a972295c7",
            "e742c431b17c4444a604d9abaee9f76f",
            "c45e038f8ece4538b8e7fc114fe93e17"
          ]
        },
        "id": "_P9bl7PAlVNO",
        "outputId": "7bbe2bb7-6718-41e0-d2af-3d096771ae37"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e469ca1328ab4199a6703ca6bce76310",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generation 1 - Current best internal CV score: -2501965699.4831247\n",
            "\n",
            "Generation 2 - Current best internal CV score: -2501965699.4831247\n",
            "\n",
            "Generation 3 - Current best internal CV score: -2495752984.350515\n",
            "\n",
            "Generation 4 - Current best internal CV score: -2436706502.728958\n",
            "\n",
            "Generation 5 - Current best internal CV score: -2290760214.474598\n",
            "\n",
            "Best pipeline: GradientBoostingRegressor(input_matrix, alpha=0.9, learning_rate=0.1, loss=huber, max_depth=7, max_features=0.6000000000000001, min_samples_leaf=8, min_samples_split=9, n_estimators=100, subsample=0.8)\n",
            "Best pipeline: Pipeline(steps=[('gradientboostingregressor',\n",
            "                 GradientBoostingRegressor(loss='huber', max_depth=7,\n",
            "                                           max_features=0.6000000000000001,\n",
            "                                           min_samples_leaf=8,\n",
            "                                           min_samples_split=9, random_state=42,\n",
            "                                           subsample=0.8))])\n",
            "Test Score: -2354376615.921508\n"
          ]
        }
      ],
      "source": [
        "from tpot import TPOTRegressor\n",
        "\n",
        "# Create and train the TPOT regressor\n",
        "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2, random_state=42)\n",
        "tpot.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Best pipeline:\", tpot.fitted_pipeline_)\n",
        "print(\"Test Score:\", tpot.score(X_test, y_test))\n",
        "\n",
        "# Export the best pipeline as Python code\n",
        "tpot.export('best_pipeline.py')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak6rtTH50CqA"
      },
      "source": [
        "Meta Learning MAML(Model-Agnostic Meta-Learning)\n",
        "Code for Few-Shot Learning with MAML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0pW4vYExPNmU",
        "outputId": "7bcbf985-8dc2-4825-e496-8d200417196a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting learn2learn\n",
            "  Downloading learn2learn-0.2.0.tar.gz (7.0 MB)\n",
            "     ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
            "     - -------------------------------------- 0.2/7.0 MB 6.7 MB/s eta 0:00:02\n",
            "     --- ------------------------------------ 0.6/7.0 MB 7.2 MB/s eta 0:00:01\n",
            "     ---- ----------------------------------- 0.8/7.0 MB 7.0 MB/s eta 0:00:01\n",
            "     ------- -------------------------------- 1.4/7.0 MB 7.9 MB/s eta 0:00:01\n",
            "     ----------- ---------------------------- 1.9/7.0 MB 8.7 MB/s eta 0:00:01\n",
            "     -------------- ------------------------- 2.5/7.0 MB 9.2 MB/s eta 0:00:01\n",
            "     --------------- ------------------------ 2.7/7.0 MB 9.0 MB/s eta 0:00:01\n",
            "     ------------------ --------------------- 3.3/7.0 MB 9.0 MB/s eta 0:00:01\n",
            "     --------------------- ------------------ 3.8/7.0 MB 9.2 MB/s eta 0:00:01\n",
            "     ------------------------ --------------- 4.3/7.0 MB 9.4 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 4.8/7.0 MB 9.5 MB/s eta 0:00:01\n",
            "     ------------------------------ --------- 5.3/7.0 MB 9.9 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 5.7/7.0 MB 9.6 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 6.2/7.0 MB 9.6 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 6.6/7.0 MB 9.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------  7.0/7.0 MB 9.7 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 7.0/7.0 MB 9.3 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: numpy>=1.15.4 in c:\\python312\\lib\\site-packages (from learn2learn) (1.26.4)\n",
            "Collecting gym>=0.14.0 (from learn2learn)\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
            "     ------------------------- ----------- 491.5/721.7 kB 10.5 MB/s eta 0:00:01\n",
            "     -------------------------------------- 721.7/721.7 kB 9.1 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: torch>=1.1.0 in c:\\python312\\lib\\site-packages (from learn2learn) (2.3.1)\n",
            "Collecting torchvision>=0.3.0 (from learn2learn)\n",
            "  Downloading torchvision-0.20.0-cp312-cp312-win_amd64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: scipy in c:\\python312\\lib\\site-packages (from learn2learn) (1.13.1)\n",
            "Requirement already satisfied: requests in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from learn2learn) (2.32.3)\n",
            "Collecting gsutil (from learn2learn)\n",
            "  Downloading gsutil-5.31.tar.gz (3.0 MB)\n",
            "     ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
            "     ------- -------------------------------- 0.6/3.0 MB 11.8 MB/s eta 0:00:01\n",
            "     ------------- -------------------------- 1.0/3.0 MB 10.5 MB/s eta 0:00:01\n",
            "     ------------------ --------------------- 1.4/3.0 MB 10.0 MB/s eta 0:00:01\n",
            "     ------------------------- -------------- 1.9/3.0 MB 10.0 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 2.4/3.0 MB 10.3 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 2.8/3.0 MB 9.9 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 3.0/3.0 MB 9.5 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting tqdm (from learn2learn)\n",
            "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
            "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
            "     ---------------------------------------- 57.6/57.6 kB 3.0 MB/s eta 0:00:00\n",
            "Collecting qpth>=0.0.15 (from learn2learn)\n",
            "  Downloading qpth-0.0.18.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting cloudpickle>=1.2.0 (from gym>=0.14.0->learn2learn)\n",
            "  Downloading cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting gym-notices>=0.0.4 (from gym>=0.14.0->learn2learn)\n",
            "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting cvxpy>=1.1.0 (from qpth>=0.0.15->learn2learn)\n",
            "  Downloading cvxpy-1.5.3-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.1.0->learn2learn) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python312\\lib\\site-packages (from torch>=1.1.0->learn2learn) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.1.0->learn2learn) (1.12.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.1.0->learn2learn) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.1.0->learn2learn) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.1.0->learn2learn) (2024.6.0)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.1.0->learn2learn) (2021.4.0)\n",
            "Collecting torch>=1.1.0 (from learn2learn)\n",
            "  Downloading torch-2.5.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\python312\\lib\\site-packages (from torchvision>=0.3.0->learn2learn) (10.3.0)\n",
            "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from torch>=1.1.0->learn2learn) (70.1.0)\n",
            "Collecting sympy==1.13.1 (from torch>=1.1.0->learn2learn)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=1.1.0->learn2learn) (1.3.0)\n",
            "Collecting argcomplete>=1.9.4 (from gsutil->learn2learn)\n",
            "  Downloading argcomplete-3.5.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting crcmod>=1.7 (from gsutil->learn2learn)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "     ---------------------------------------- 0.0/89.7 kB ? eta -:--:--\n",
            "     ---------------------------------------- 89.7/89.7 kB 5.0 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting fasteners>=0.14.1 (from gsutil->learn2learn)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting gcs-oauth2-boto-plugin>=3.2 (from gsutil->learn2learn)\n",
            "  Downloading gcs-oauth2-boto-plugin-3.2.tar.gz (22 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting google-apitools>=0.5.32 (from gsutil->learn2learn)\n",
            "  Downloading google_apitools-0.5.32-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting httplib2==0.20.4 (from gsutil->learn2learn)\n",
            "  Downloading httplib2-0.20.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting google-reauth>=0.1.0 (from gsutil->learn2learn)\n",
            "  Downloading google_reauth-0.1.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting monotonic>=1.4 (from gsutil->learn2learn)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyOpenSSL>=0.13 (from gsutil->learn2learn)\n",
            "  Downloading pyOpenSSL-24.2.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting retry_decorator>=1.0.0 (from gsutil->learn2learn)\n",
            "  Downloading retry_decorator-1.1.1.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: six>=1.16.0 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from gsutil->learn2learn) (1.16.0)\n",
            "Collecting google-auth==2.17.0 (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Downloading google_auth-2.17.0-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting google-auth-httplib2>=0.2.0 (from gsutil->learn2learn)\n",
            "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting aiohttp<4.0.0dev,>=3.6.2 (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Downloading aiohttp-3.10.10-cp312-cp312-win_amd64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\python312\\lib\\site-packages (from httplib2==0.20.4->gsutil->learn2learn) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from requests->learn2learn) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests->learn2learn) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests->learn2learn) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from requests->learn2learn) (2024.6.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->learn2learn) (0.4.6)\n",
            "Collecting osqp>=0.6.2 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn)\n",
            "  Downloading osqp-0.6.7.post3-cp312-cp312-win_amd64.whl.metadata (2.0 kB)\n",
            "Collecting ecos>=2 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn)\n",
            "  Downloading ecos-2.0.14-cp312-cp312-win_amd64.whl.metadata (8.2 kB)\n",
            "Collecting clarabel>=0.5.0 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn)\n",
            "  Downloading clarabel-0.9.0-cp37-abi3-win_amd64.whl.metadata (4.8 kB)\n",
            "Collecting scs>=3.2.4.post1 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn)\n",
            "  Downloading scs-3.2.7-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting boto>=2.29.1 (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn)\n",
            "  Downloading boto-2.49.0-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting oauth2client>=2.2.0 (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn)\n",
            "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pyu2f (from google-reauth>=0.1.0->gsutil->learn2learn)\n",
            "  Downloading pyu2f-0.1.5.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting cryptography<44,>=41.0.5 (from pyOpenSSL>=0.13->gsutil->learn2learn)\n",
            "  Downloading cryptography-43.0.1-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch>=1.1.0->learn2learn) (2.1.5)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Downloading frozenlist-1.4.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
            "Collecting yarl<2.0,>=1.12.0 (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Downloading yarl-1.15.4-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
            "     ---------------------------------------- 0.0/60.0 kB ? eta -:--:--\n",
            "     ---------------------------------------- 60.0/60.0 kB 3.1 MB/s eta 0:00:00\n",
            "Collecting cffi>=1.12 (from cryptography<44,>=41.0.5->pyOpenSSL>=0.13->gsutil->learn2learn)\n",
            "  Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
            "Collecting qdldl (from osqp>=0.6.2->cvxpy>=1.1.0->qpth>=0.0.15->learn2learn)\n",
            "  Downloading qdldl-0.1.7.post4-cp312-cp312-win_amd64.whl.metadata (1.8 kB)\n",
            "Collecting pycparser (from cffi>=1.12->cryptography<44,>=41.0.5->pyOpenSSL>=0.13->gsutil->learn2learn)\n",
            "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
            "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Downloading propcache-0.2.0-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.1.0->learn2learn) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.1.0->learn2learn) (2021.12.0)\n",
            "Downloading torchvision-0.20.0-cp312-cp312-win_amd64.whl (1.6 MB)\n",
            "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
            "   ------------ --------------------------- 0.5/1.6 MB 15.9 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 0.9/1.6 MB 14.5 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 1.4/1.6 MB 11.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.6/1.6 MB 10.0 MB/s eta 0:00:00\n",
            "Downloading torch-2.5.0-cp312-cp312-win_amd64.whl (203.1 MB)\n",
            "   ---------------------------------------- 0.0/203.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.6/203.1 MB 12.2 MB/s eta 0:00:17\n",
            "   ---------------------------------------- 1.0/203.1 MB 10.9 MB/s eta 0:00:19\n",
            "   ---------------------------------------- 1.5/203.1 MB 10.5 MB/s eta 0:00:20\n",
            "   ---------------------------------------- 2.0/203.1 MB 10.8 MB/s eta 0:00:19\n",
            "    --------------------------------------- 2.6/203.1 MB 11.0 MB/s eta 0:00:19\n",
            "    --------------------------------------- 3.1/203.1 MB 11.1 MB/s eta 0:00:19\n",
            "    --------------------------------------- 3.6/203.1 MB 11.0 MB/s eta 0:00:19\n",
            "    --------------------------------------- 4.1/203.1 MB 11.5 MB/s eta 0:00:18\n",
            "    --------------------------------------- 4.7/203.1 MB 11.5 MB/s eta 0:00:18\n",
            "   - -------------------------------------- 5.2/203.1 MB 11.6 MB/s eta 0:00:18\n",
            "   - -------------------------------------- 5.8/203.1 MB 11.5 MB/s eta 0:00:18\n",
            "   - -------------------------------------- 6.3/203.1 MB 11.4 MB/s eta 0:00:18\n",
            "   - -------------------------------------- 6.8/203.1 MB 11.4 MB/s eta 0:00:18\n",
            "   - -------------------------------------- 7.3/203.1 MB 11.4 MB/s eta 0:00:18\n",
            "   - -------------------------------------- 7.8/203.1 MB 11.4 MB/s eta 0:00:18\n",
            "   - -------------------------------------- 8.4/203.1 MB 11.4 MB/s eta 0:00:18\n",
            "   - -------------------------------------- 8.9/203.1 MB 11.4 MB/s eta 0:00:18\n",
            "   - -------------------------------------- 9.4/203.1 MB 11.4 MB/s eta 0:00:18\n",
            "   - -------------------------------------- 10.0/203.1 MB 11.4 MB/s eta 0:00:17\n",
            "   -- ------------------------------------- 10.5/203.1 MB 11.3 MB/s eta 0:00:18\n",
            "   -- ------------------------------------- 11.0/203.1 MB 11.3 MB/s eta 0:00:18\n",
            "   -- ------------------------------------- 11.5/203.1 MB 11.5 MB/s eta 0:00:17\n",
            "   -- ------------------------------------- 12.1/203.1 MB 11.5 MB/s eta 0:00:17\n",
            "   -- ------------------------------------- 12.6/203.1 MB 11.3 MB/s eta 0:00:17\n",
            "   -- ------------------------------------- 13.1/203.1 MB 11.7 MB/s eta 0:00:17\n",
            "   -- ------------------------------------- 13.7/203.1 MB 11.5 MB/s eta 0:00:17\n",
            "   -- ------------------------------------- 14.3/203.1 MB 11.5 MB/s eta 0:00:17\n",
            "   -- ------------------------------------- 14.8/203.1 MB 11.7 MB/s eta 0:00:17\n",
            "   --- ------------------------------------ 15.3/203.1 MB 11.7 MB/s eta 0:00:17\n",
            "   --- ------------------------------------ 15.9/203.1 MB 11.7 MB/s eta 0:00:17\n",
            "   --- ------------------------------------ 16.3/203.1 MB 11.5 MB/s eta 0:00:17\n",
            "   --- ------------------------------------ 16.9/203.1 MB 11.7 MB/s eta 0:00:16\n",
            "   --- ------------------------------------ 17.4/203.1 MB 11.7 MB/s eta 0:00:16\n",
            "   --- ------------------------------------ 18.0/203.1 MB 11.7 MB/s eta 0:00:16\n",
            "   --- ------------------------------------ 18.6/203.1 MB 11.7 MB/s eta 0:00:16\n",
            "   --- ------------------------------------ 19.1/203.1 MB 11.7 MB/s eta 0:00:16\n",
            "   --- ------------------------------------ 19.7/203.1 MB 11.7 MB/s eta 0:00:16\n",
            "   --- ------------------------------------ 20.2/203.1 MB 11.7 MB/s eta 0:00:16\n",
            "   ---- ----------------------------------- 20.8/203.1 MB 11.7 MB/s eta 0:00:16\n",
            "   ---- ----------------------------------- 21.3/203.1 MB 11.7 MB/s eta 0:00:16\n",
            "   ---- ----------------------------------- 21.9/203.1 MB 11.7 MB/s eta 0:00:16\n",
            "   ---- ----------------------------------- 22.4/203.1 MB 11.7 MB/s eta 0:00:16\n",
            "   ---- ----------------------------------- 22.7/203.1 MB 11.9 MB/s eta 0:00:16\n",
            "   ---- ----------------------------------- 23.4/203.1 MB 11.7 MB/s eta 0:00:16\n",
            "   ---- ----------------------------------- 23.8/203.1 MB 11.7 MB/s eta 0:00:16\n",
            "   ---- ----------------------------------- 24.2/203.1 MB 11.5 MB/s eta 0:00:16\n",
            "   ---- ----------------------------------- 24.7/203.1 MB 11.5 MB/s eta 0:00:16\n",
            "   ---- ----------------------------------- 25.0/203.1 MB 11.1 MB/s eta 0:00:17\n",
            "   ----- ---------------------------------- 25.5/203.1 MB 10.9 MB/s eta 0:00:17\n",
            "   ----- ---------------------------------- 26.0/203.1 MB 11.1 MB/s eta 0:00:16\n",
            "   ----- ---------------------------------- 26.5/203.1 MB 11.1 MB/s eta 0:00:16\n",
            "   ----- ---------------------------------- 27.1/203.1 MB 11.1 MB/s eta 0:00:16\n",
            "   ----- ---------------------------------- 27.6/203.1 MB 11.1 MB/s eta 0:00:16\n",
            "   ----- ---------------------------------- 28.0/203.1 MB 10.9 MB/s eta 0:00:17\n",
            "   ----- ---------------------------------- 28.5/203.1 MB 10.9 MB/s eta 0:00:17\n",
            "   ----- ---------------------------------- 29.1/203.1 MB 10.9 MB/s eta 0:00:16\n",
            "   ----- ---------------------------------- 29.5/203.1 MB 10.7 MB/s eta 0:00:17\n",
            "   ----- ---------------------------------- 29.8/203.1 MB 10.7 MB/s eta 0:00:17\n",
            "   ----- ---------------------------------- 30.2/203.1 MB 10.4 MB/s eta 0:00:17\n",
            "   ------ --------------------------------- 30.7/203.1 MB 10.2 MB/s eta 0:00:17\n",
            "   ------ --------------------------------- 31.2/203.1 MB 10.2 MB/s eta 0:00:17\n",
            "   ------ --------------------------------- 31.8/203.1 MB 10.2 MB/s eta 0:00:17\n",
            "   ------ --------------------------------- 32.2/203.1 MB 10.1 MB/s eta 0:00:17\n",
            "   ------ --------------------------------- 32.5/203.1 MB 10.1 MB/s eta 0:00:17\n",
            "   ------ --------------------------------- 32.9/203.1 MB 9.8 MB/s eta 0:00:18\n",
            "   ------ --------------------------------- 33.2/203.1 MB 9.6 MB/s eta 0:00:18\n",
            "   ------ --------------------------------- 33.5/203.1 MB 9.5 MB/s eta 0:00:18\n",
            "   ------ --------------------------------- 33.9/203.1 MB 9.5 MB/s eta 0:00:18\n",
            "   ------ --------------------------------- 34.3/203.1 MB 9.4 MB/s eta 0:00:19\n",
            "   ------ --------------------------------- 34.8/203.1 MB 9.6 MB/s eta 0:00:18\n",
            "   ------ --------------------------------- 35.4/203.1 MB 9.8 MB/s eta 0:00:18\n",
            "   ------- -------------------------------- 35.9/203.1 MB 9.8 MB/s eta 0:00:18\n",
            "   ------- -------------------------------- 36.4/203.1 MB 9.8 MB/s eta 0:00:18\n",
            "   ------- -------------------------------- 36.9/203.1 MB 9.8 MB/s eta 0:00:18\n",
            "   ------- -------------------------------- 37.4/203.1 MB 9.8 MB/s eta 0:00:17\n",
            "   ------- -------------------------------- 38.0/203.1 MB 9.9 MB/s eta 0:00:17\n",
            "   ------- -------------------------------- 38.6/203.1 MB 9.9 MB/s eta 0:00:17\n",
            "   ------- -------------------------------- 39.1/203.1 MB 9.9 MB/s eta 0:00:17\n",
            "   ------- -------------------------------- 39.5/203.1 MB 9.9 MB/s eta 0:00:17\n",
            "   ------- -------------------------------- 40.1/203.1 MB 10.1 MB/s eta 0:00:17\n",
            "   ------- -------------------------------- 40.5/203.1 MB 10.4 MB/s eta 0:00:16\n",
            "   -------- ------------------------------- 41.0/203.1 MB 10.4 MB/s eta 0:00:16\n",
            "   -------- ------------------------------- 41.6/203.1 MB 10.2 MB/s eta 0:00:16\n",
            "   -------- ------------------------------- 42.1/203.1 MB 10.2 MB/s eta 0:00:16\n",
            "   -------- ------------------------------- 42.5/203.1 MB 10.4 MB/s eta 0:00:16\n",
            "   -------- ------------------------------- 43.0/203.1 MB 10.6 MB/s eta 0:00:16\n",
            "   -------- ------------------------------- 43.5/203.1 MB 10.7 MB/s eta 0:00:15\n",
            "   -------- ------------------------------- 44.0/203.1 MB 11.1 MB/s eta 0:00:15\n",
            "   -------- ------------------------------- 44.4/203.1 MB 11.1 MB/s eta 0:00:15\n",
            "   -------- ------------------------------- 44.8/203.1 MB 10.9 MB/s eta 0:00:15\n",
            "   -------- ------------------------------- 45.1/203.1 MB 10.7 MB/s eta 0:00:15\n",
            "   -------- ------------------------------- 45.5/203.1 MB 10.6 MB/s eta 0:00:15\n",
            "   --------- ------------------------------ 46.0/203.1 MB 10.6 MB/s eta 0:00:15\n",
            "   --------- ------------------------------ 46.4/203.1 MB 10.4 MB/s eta 0:00:16\n",
            "   --------- ------------------------------ 46.8/203.1 MB 10.2 MB/s eta 0:00:16\n",
            "   --------- ------------------------------ 47.2/203.1 MB 10.1 MB/s eta 0:00:16\n",
            "   --------- ------------------------------ 47.7/203.1 MB 10.1 MB/s eta 0:00:16\n",
            "   --------- ------------------------------ 48.1/203.1 MB 9.9 MB/s eta 0:00:16\n",
            "   --------- ------------------------------ 48.5/203.1 MB 9.8 MB/s eta 0:00:16\n",
            "   --------- ------------------------------ 48.9/203.1 MB 9.8 MB/s eta 0:00:16\n",
            "   --------- ------------------------------ 49.4/203.1 MB 9.8 MB/s eta 0:00:16\n",
            "   --------- ------------------------------ 49.8/203.1 MB 9.8 MB/s eta 0:00:16\n",
            "   --------- ------------------------------ 50.2/203.1 MB 9.6 MB/s eta 0:00:16\n",
            "   --------- ------------------------------ 50.6/203.1 MB 9.6 MB/s eta 0:00:16\n",
            "   ---------- ----------------------------- 51.1/203.1 MB 9.6 MB/s eta 0:00:16\n",
            "   ---------- ----------------------------- 51.6/203.1 MB 9.5 MB/s eta 0:00:16\n",
            "   ---------- ----------------------------- 52.0/203.1 MB 9.5 MB/s eta 0:00:16\n",
            "   ---------- ----------------------------- 52.5/203.1 MB 9.5 MB/s eta 0:00:16\n",
            "   ---------- ----------------------------- 53.0/203.1 MB 9.5 MB/s eta 0:00:16\n",
            "   ---------- ----------------------------- 53.5/203.1 MB 9.5 MB/s eta 0:00:16\n",
            "   ---------- ----------------------------- 53.9/203.1 MB 9.4 MB/s eta 0:00:16\n",
            "   ---------- ----------------------------- 54.4/203.1 MB 9.5 MB/s eta 0:00:16\n",
            "   ---------- ----------------------------- 54.8/203.1 MB 9.4 MB/s eta 0:00:16\n",
            "   ---------- ----------------------------- 55.3/203.1 MB 9.5 MB/s eta 0:00:16\n",
            "   ---------- ----------------------------- 55.8/203.1 MB 9.6 MB/s eta 0:00:16\n",
            "   ----------- ---------------------------- 56.3/203.1 MB 9.8 MB/s eta 0:00:16\n",
            "   ----------- ---------------------------- 56.8/203.1 MB 9.9 MB/s eta 0:00:15\n",
            "   ----------- ---------------------------- 57.3/203.1 MB 10.1 MB/s eta 0:00:15\n",
            "   ----------- ---------------------------- 57.7/203.1 MB 9.9 MB/s eta 0:00:15\n",
            "   ----------- ---------------------------- 58.0/203.1 MB 9.9 MB/s eta 0:00:15\n",
            "   ----------- ---------------------------- 58.4/203.1 MB 9.8 MB/s eta 0:00:15\n",
            "   ----------- ---------------------------- 58.8/203.1 MB 9.8 MB/s eta 0:00:15\n",
            "   ----------- ---------------------------- 59.2/203.1 MB 9.8 MB/s eta 0:00:15\n",
            "   ----------- ---------------------------- 59.8/203.1 MB 9.9 MB/s eta 0:00:15\n",
            "   ----------- ---------------------------- 60.3/203.1 MB 9.9 MB/s eta 0:00:15\n",
            "   ----------- ---------------------------- 60.8/203.1 MB 10.1 MB/s eta 0:00:15\n",
            "   ------------ --------------------------- 61.1/203.1 MB 9.9 MB/s eta 0:00:15\n",
            "   ------------ --------------------------- 61.4/203.1 MB 9.8 MB/s eta 0:00:15\n",
            "   ------------ --------------------------- 61.8/203.1 MB 9.6 MB/s eta 0:00:15\n",
            "   ------------ --------------------------- 62.4/203.1 MB 9.8 MB/s eta 0:00:15\n",
            "   ------------ --------------------------- 62.9/203.1 MB 9.9 MB/s eta 0:00:15\n",
            "   ------------ --------------------------- 63.4/203.1 MB 9.9 MB/s eta 0:00:15\n",
            "   ------------ --------------------------- 63.9/203.1 MB 9.9 MB/s eta 0:00:15\n",
            "   ------------ --------------------------- 64.5/203.1 MB 10.1 MB/s eta 0:00:14\n",
            "   ------------ --------------------------- 65.0/203.1 MB 10.2 MB/s eta 0:00:14\n",
            "   ------------ --------------------------- 65.5/203.1 MB 10.2 MB/s eta 0:00:14\n",
            "   ------------- -------------------------- 66.0/203.1 MB 10.2 MB/s eta 0:00:14\n",
            "   ------------- -------------------------- 66.6/203.1 MB 10.2 MB/s eta 0:00:14\n",
            "   ------------- -------------------------- 67.0/203.1 MB 10.2 MB/s eta 0:00:14\n",
            "   ------------- -------------------------- 67.6/203.1 MB 10.2 MB/s eta 0:00:14\n",
            "   ------------- -------------------------- 68.1/203.1 MB 10.6 MB/s eta 0:00:13\n",
            "   ------------- -------------------------- 68.7/203.1 MB 10.7 MB/s eta 0:00:13\n",
            "   ------------- -------------------------- 69.2/203.1 MB 10.9 MB/s eta 0:00:13\n",
            "   ------------- -------------------------- 69.7/203.1 MB 10.9 MB/s eta 0:00:13\n",
            "   ------------- -------------------------- 70.2/203.1 MB 10.7 MB/s eta 0:00:13\n",
            "   ------------- -------------------------- 70.7/203.1 MB 10.7 MB/s eta 0:00:13\n",
            "   -------------- ------------------------- 71.2/203.1 MB 10.7 MB/s eta 0:00:13\n",
            "   -------------- ------------------------- 71.7/203.1 MB 11.1 MB/s eta 0:00:12\n",
            "   -------------- ------------------------- 72.2/203.1 MB 11.3 MB/s eta 0:00:12\n",
            "   -------------- ------------------------- 72.6/203.1 MB 11.3 MB/s eta 0:00:12\n",
            "   -------------- ------------------------- 73.2/203.1 MB 11.1 MB/s eta 0:00:12\n",
            "   -------------- ------------------------- 73.7/203.1 MB 11.1 MB/s eta 0:00:12\n",
            "   -------------- ------------------------- 74.1/203.1 MB 10.9 MB/s eta 0:00:12\n",
            "   -------------- ------------------------- 74.6/203.1 MB 11.1 MB/s eta 0:00:12\n",
            "   -------------- ------------------------- 75.2/203.1 MB 11.1 MB/s eta 0:00:12\n",
            "   -------------- ------------------------- 75.8/203.1 MB 11.3 MB/s eta 0:00:12\n",
            "   --------------- ------------------------ 76.2/203.1 MB 11.1 MB/s eta 0:00:12\n",
            "   --------------- ------------------------ 76.7/203.1 MB 11.1 MB/s eta 0:00:12\n",
            "   --------------- ------------------------ 77.3/203.1 MB 11.1 MB/s eta 0:00:12\n",
            "   --------------- ------------------------ 77.8/203.1 MB 11.1 MB/s eta 0:00:12\n",
            "   --------------- ------------------------ 78.3/203.1 MB 11.1 MB/s eta 0:00:12\n",
            "   --------------- ------------------------ 78.9/203.1 MB 11.1 MB/s eta 0:00:12\n",
            "   --------------- ------------------------ 79.4/203.1 MB 11.1 MB/s eta 0:00:12\n",
            "   --------------- ------------------------ 80.0/203.1 MB 11.1 MB/s eta 0:00:12\n",
            "   --------------- ------------------------ 80.5/203.1 MB 11.3 MB/s eta 0:00:11\n",
            "   --------------- ------------------------ 81.0/203.1 MB 11.1 MB/s eta 0:00:11\n",
            "   ---------------- ----------------------- 81.6/203.1 MB 11.3 MB/s eta 0:00:11\n",
            "   ---------------- ----------------------- 82.1/203.1 MB 11.3 MB/s eta 0:00:11\n",
            "   ---------------- ----------------------- 82.7/203.1 MB 11.3 MB/s eta 0:00:11\n",
            "   ---------------- ----------------------- 83.2/203.1 MB 11.5 MB/s eta 0:00:11\n",
            "   ---------------- ----------------------- 83.8/203.1 MB 11.5 MB/s eta 0:00:11\n",
            "   ---------------- ----------------------- 84.3/203.1 MB 11.7 MB/s eta 0:00:11\n",
            "   ---------------- ----------------------- 84.9/203.1 MB 11.5 MB/s eta 0:00:11\n",
            "   ---------------- ----------------------- 85.4/203.1 MB 11.5 MB/s eta 0:00:11\n",
            "   ---------------- ----------------------- 85.8/203.1 MB 11.7 MB/s eta 0:00:11\n",
            "   ----------------- ---------------------- 86.3/203.1 MB 11.7 MB/s eta 0:00:10\n",
            "   ----------------- ---------------------- 86.8/203.1 MB 11.5 MB/s eta 0:00:11\n",
            "   ----------------- ---------------------- 87.1/203.1 MB 11.3 MB/s eta 0:00:11\n",
            "   ----------------- ---------------------- 87.5/203.1 MB 11.1 MB/s eta 0:00:11\n",
            "   ----------------- ---------------------- 87.9/203.1 MB 11.1 MB/s eta 0:00:11\n",
            "   ----------------- ---------------------- 88.4/203.1 MB 10.9 MB/s eta 0:00:11\n",
            "   ----------------- ---------------------- 88.9/203.1 MB 10.9 MB/s eta 0:00:11\n",
            "   ----------------- ---------------------- 89.4/203.1 MB 10.9 MB/s eta 0:00:11\n",
            "   ----------------- ---------------------- 89.9/203.1 MB 10.7 MB/s eta 0:00:11\n",
            "   ----------------- ---------------------- 90.4/203.1 MB 10.7 MB/s eta 0:00:11\n",
            "   ----------------- ---------------------- 91.0/203.1 MB 10.7 MB/s eta 0:00:11\n",
            "   ------------------ --------------------- 91.5/203.1 MB 10.7 MB/s eta 0:00:11\n",
            "   ------------------ --------------------- 92.0/203.1 MB 10.7 MB/s eta 0:00:11\n",
            "   ------------------ --------------------- 92.6/203.1 MB 10.7 MB/s eta 0:00:11\n",
            "   ------------------ --------------------- 93.1/203.1 MB 10.7 MB/s eta 0:00:11\n",
            "   ------------------ --------------------- 93.6/203.1 MB 10.7 MB/s eta 0:00:11\n",
            "   ------------------ --------------------- 94.1/203.1 MB 10.9 MB/s eta 0:00:10\n",
            "   ------------------ --------------------- 94.7/203.1 MB 10.4 MB/s eta 0:00:11\n",
            "   ------------------ --------------------- 95.3/203.1 MB 10.6 MB/s eta 0:00:11\n",
            "   ------------------ --------------------- 95.9/203.1 MB 10.1 MB/s eta 0:00:11\n",
            "   ------------------ --------------------- 96.2/203.1 MB 10.1 MB/s eta 0:00:11\n",
            "   ------------------- -------------------- 96.9/203.1 MB 10.1 MB/s eta 0:00:11\n",
            "   ------------------- -------------------- 97.5/203.1 MB 10.2 MB/s eta 0:00:11\n",
            "   ------------------- -------------------- 98.0/203.1 MB 10.6 MB/s eta 0:00:10\n",
            "   ------------------- -------------------- 98.6/203.1 MB 10.6 MB/s eta 0:00:10\n",
            "   ------------------- -------------------- 99.1/203.1 MB 10.6 MB/s eta 0:00:10\n",
            "   ------------------- -------------------- 99.7/203.1 MB 10.6 MB/s eta 0:00:10\n",
            "   ------------------- ------------------- 100.2/203.1 MB 10.7 MB/s eta 0:00:10\n",
            "   ------------------- ------------------- 100.8/203.1 MB 10.6 MB/s eta 0:00:10\n",
            "   ------------------- ------------------- 101.3/203.1 MB 10.7 MB/s eta 0:00:10\n",
            "   ------------------- ------------------- 101.8/203.1 MB 10.7 MB/s eta 0:00:10\n",
            "   ------------------- ------------------- 102.3/203.1 MB 10.6 MB/s eta 0:00:10\n",
            "   ------------------- ------------------- 102.9/203.1 MB 10.7 MB/s eta 0:00:10\n",
            "   ------------------- ------------------- 103.4/203.1 MB 10.7 MB/s eta 0:00:10\n",
            "   ------------------- ------------------- 103.7/203.1 MB 10.2 MB/s eta 0:00:10\n",
            "   -------------------- ------------------ 104.4/203.1 MB 10.1 MB/s eta 0:00:10\n",
            "   -------------------- ------------------ 104.8/203.1 MB 10.2 MB/s eta 0:00:10\n",
            "   -------------------- ------------------ 105.2/203.1 MB 10.2 MB/s eta 0:00:10\n",
            "   -------------------- ------------------ 105.7/203.1 MB 10.6 MB/s eta 0:00:10\n",
            "   -------------------- ------------------ 106.3/203.1 MB 10.6 MB/s eta 0:00:10\n",
            "   -------------------- ------------------ 106.9/203.1 MB 10.6 MB/s eta 0:00:10\n",
            "   -------------------- ------------------ 107.4/203.1 MB 10.6 MB/s eta 0:00:10\n",
            "   -------------------- ------------------ 107.9/203.1 MB 10.7 MB/s eta 0:00:09\n",
            "   -------------------- ------------------ 108.5/203.1 MB 10.6 MB/s eta 0:00:09\n",
            "   -------------------- ------------------ 109.0/203.1 MB 10.7 MB/s eta 0:00:09\n",
            "   --------------------- ----------------- 109.5/203.1 MB 10.6 MB/s eta 0:00:09\n",
            "   --------------------- ----------------- 110.1/203.1 MB 10.7 MB/s eta 0:00:09\n",
            "   --------------------- ----------------- 110.6/203.1 MB 10.7 MB/s eta 0:00:09\n",
            "   --------------------- ----------------- 111.1/203.1 MB 10.7 MB/s eta 0:00:09\n",
            "   --------------------- ----------------- 111.7/203.1 MB 10.7 MB/s eta 0:00:09\n",
            "   --------------------- ----------------- 112.3/203.1 MB 10.7 MB/s eta 0:00:09\n",
            "   --------------------- ----------------- 112.8/203.1 MB 10.7 MB/s eta 0:00:09\n",
            "   --------------------- ----------------- 113.4/203.1 MB 10.7 MB/s eta 0:00:09\n",
            "   --------------------- ----------------- 113.9/203.1 MB 10.9 MB/s eta 0:00:09\n",
            "   --------------------- ----------------- 114.0/203.1 MB 10.7 MB/s eta 0:00:09\n",
            "   --------------------- ----------------- 114.4/203.1 MB 10.7 MB/s eta 0:00:09\n",
            "   ---------------------- ---------------- 114.9/203.1 MB 10.9 MB/s eta 0:00:09\n",
            "   ---------------------- ---------------- 115.5/203.1 MB 11.1 MB/s eta 0:00:08\n",
            "   ---------------------- ---------------- 116.0/203.1 MB 11.1 MB/s eta 0:00:08\n",
            "   ---------------------- ---------------- 116.6/203.1 MB 11.1 MB/s eta 0:00:08\n",
            "   ---------------------- ---------------- 117.2/203.1 MB 11.3 MB/s eta 0:00:08\n",
            "   ---------------------- ---------------- 117.7/203.1 MB 11.1 MB/s eta 0:00:08\n",
            "   ---------------------- ---------------- 118.3/203.1 MB 11.1 MB/s eta 0:00:08\n",
            "   ---------------------- ---------------- 118.8/203.1 MB 11.1 MB/s eta 0:00:08\n",
            "   ---------------------- ---------------- 119.3/203.1 MB 11.1 MB/s eta 0:00:08\n",
            "   ----------------------- --------------- 119.9/203.1 MB 11.1 MB/s eta 0:00:08\n",
            "   ----------------------- --------------- 120.4/203.1 MB 11.1 MB/s eta 0:00:08\n",
            "   ----------------------- --------------- 121.0/203.1 MB 11.1 MB/s eta 0:00:08\n",
            "   ----------------------- --------------- 121.5/203.1 MB 11.1 MB/s eta 0:00:08\n",
            "   ----------------------- --------------- 122.1/203.1 MB 11.3 MB/s eta 0:00:08\n",
            "   ----------------------- --------------- 122.6/203.1 MB 11.1 MB/s eta 0:00:08\n",
            "   ----------------------- --------------- 123.1/203.1 MB 11.3 MB/s eta 0:00:08\n",
            "   ----------------------- --------------- 123.7/203.1 MB 11.3 MB/s eta 0:00:08\n",
            "   ----------------------- --------------- 124.3/203.1 MB 11.9 MB/s eta 0:00:07\n",
            "   ----------------------- --------------- 124.8/203.1 MB 11.9 MB/s eta 0:00:07\n",
            "   ------------------------ -------------- 125.3/203.1 MB 11.9 MB/s eta 0:00:07\n",
            "   ------------------------ -------------- 125.8/203.1 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------ -------------- 126.4/203.1 MB 11.9 MB/s eta 0:00:07\n",
            "   ------------------------ -------------- 126.9/203.1 MB 11.9 MB/s eta 0:00:07\n",
            "   ------------------------ -------------- 127.4/203.1 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------ -------------- 128.0/203.1 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------ -------------- 128.5/203.1 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------ -------------- 129.1/203.1 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------ -------------- 129.6/203.1 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------ -------------- 130.1/203.1 MB 11.9 MB/s eta 0:00:07\n",
            "   ------------------------- ------------- 130.6/203.1 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------- ------------- 131.2/203.1 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------- ------------- 131.7/203.1 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------- ------------- 132.2/203.1 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------- ------------- 132.8/203.1 MB 11.7 MB/s eta 0:00:07\n",
            "   ------------------------- ------------- 133.3/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   ------------------------- ------------- 133.9/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   ------------------------- ------------- 134.4/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   ------------------------- ------------- 134.9/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------- ------------ 135.4/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------- ------------ 136.0/203.1 MB 11.5 MB/s eta 0:00:06\n",
            "   -------------------------- ------------ 136.5/203.1 MB 11.5 MB/s eta 0:00:06\n",
            "   -------------------------- ------------ 137.0/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------- ------------ 137.6/203.1 MB 11.5 MB/s eta 0:00:06\n",
            "   -------------------------- ------------ 138.1/203.1 MB 11.5 MB/s eta 0:00:06\n",
            "   -------------------------- ------------ 138.6/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------- ------------ 139.1/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------- ------------ 139.7/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   -------------------------- ------------ 140.2/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   --------------------------- ----------- 140.8/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   --------------------------- ----------- 141.3/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   --------------------------- ----------- 141.9/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   --------------------------- ----------- 142.4/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   --------------------------- ----------- 143.0/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   --------------------------- ----------- 143.5/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   --------------------------- ----------- 144.0/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   --------------------------- ----------- 144.6/203.1 MB 11.7 MB/s eta 0:00:06\n",
            "   --------------------------- ----------- 145.1/203.1 MB 11.7 MB/s eta 0:00:05\n",
            "   --------------------------- ----------- 145.7/203.1 MB 11.7 MB/s eta 0:00:05\n",
            "   ---------------------------- ---------- 146.2/203.1 MB 11.7 MB/s eta 0:00:05\n",
            "   ---------------------------- ---------- 146.7/203.1 MB 11.7 MB/s eta 0:00:05\n",
            "   ---------------------------- ---------- 147.3/203.1 MB 11.7 MB/s eta 0:00:05\n",
            "   ---------------------------- ---------- 147.9/203.1 MB 11.7 MB/s eta 0:00:05\n",
            "   ---------------------------- ---------- 148.3/203.1 MB 11.9 MB/s eta 0:00:05\n",
            "   ---------------------------- ---------- 148.9/203.1 MB 11.9 MB/s eta 0:00:05\n",
            "   ---------------------------- ---------- 149.4/203.1 MB 11.9 MB/s eta 0:00:05\n",
            "   ---------------------------- ---------- 149.9/203.1 MB 11.7 MB/s eta 0:00:05\n",
            "   ---------------------------- ---------- 150.4/203.1 MB 11.7 MB/s eta 0:00:05\n",
            "   ---------------------------- ---------- 150.8/203.1 MB 11.5 MB/s eta 0:00:05\n",
            "   ----------------------------- --------- 151.3/203.1 MB 11.5 MB/s eta 0:00:05\n",
            "   ----------------------------- --------- 151.9/203.1 MB 11.5 MB/s eta 0:00:05\n",
            "   ----------------------------- --------- 152.2/203.1 MB 11.3 MB/s eta 0:00:05\n",
            "   ----------------------------- --------- 152.7/203.1 MB 11.1 MB/s eta 0:00:05\n",
            "   ----------------------------- --------- 153.2/203.1 MB 11.1 MB/s eta 0:00:05\n",
            "   ----------------------------- --------- 153.8/203.1 MB 11.1 MB/s eta 0:00:05\n",
            "   ----------------------------- --------- 154.3/203.1 MB 11.1 MB/s eta 0:00:05\n",
            "   ----------------------------- --------- 154.8/203.1 MB 11.3 MB/s eta 0:00:05\n",
            "   ----------------------------- --------- 155.4/203.1 MB 11.3 MB/s eta 0:00:05\n",
            "   ----------------------------- --------- 155.9/203.1 MB 11.3 MB/s eta 0:00:05\n",
            "   ------------------------------ -------- 156.5/203.1 MB 11.3 MB/s eta 0:00:05\n",
            "   ------------------------------ -------- 157.0/203.1 MB 11.3 MB/s eta 0:00:05\n",
            "   ------------------------------ -------- 157.6/203.1 MB 11.3 MB/s eta 0:00:05\n",
            "   ------------------------------ -------- 158.1/203.1 MB 11.3 MB/s eta 0:00:04\n",
            "   ------------------------------ -------- 158.6/203.1 MB 11.3 MB/s eta 0:00:04\n",
            "   ------------------------------ -------- 159.2/203.1 MB 11.3 MB/s eta 0:00:04\n",
            "   ------------------------------ -------- 159.7/203.1 MB 11.3 MB/s eta 0:00:04\n",
            "   ------------------------------ -------- 160.2/203.1 MB 11.3 MB/s eta 0:00:04\n",
            "   ------------------------------ -------- 160.8/203.1 MB 11.3 MB/s eta 0:00:04\n",
            "   ------------------------------ -------- 161.3/203.1 MB 11.5 MB/s eta 0:00:04\n",
            "   ------------------------------- ------- 161.9/203.1 MB 11.5 MB/s eta 0:00:04\n",
            "   ------------------------------- ------- 162.3/203.1 MB 11.7 MB/s eta 0:00:04\n",
            "   ------------------------------- ------- 163.0/203.1 MB 11.7 MB/s eta 0:00:04\n",
            "   ------------------------------- ------- 163.5/203.1 MB 11.7 MB/s eta 0:00:04\n",
            "   ------------------------------- ------- 164.0/203.1 MB 11.7 MB/s eta 0:00:04\n",
            "   ------------------------------- ------- 164.5/203.1 MB 11.9 MB/s eta 0:00:04\n",
            "   ------------------------------- ------- 165.1/203.1 MB 11.9 MB/s eta 0:00:04\n",
            "   ------------------------------- ------- 165.6/203.1 MB 11.5 MB/s eta 0:00:04\n",
            "   ------------------------------- ------- 166.1/203.1 MB 11.7 MB/s eta 0:00:04\n",
            "   -------------------------------- ------ 166.7/203.1 MB 11.7 MB/s eta 0:00:04\n",
            "   -------------------------------- ------ 167.2/203.1 MB 11.7 MB/s eta 0:00:04\n",
            "   -------------------------------- ------ 167.8/203.1 MB 11.7 MB/s eta 0:00:04\n",
            "   -------------------------------- ------ 168.3/203.1 MB 11.7 MB/s eta 0:00:03\n",
            "   -------------------------------- ------ 168.9/203.1 MB 11.7 MB/s eta 0:00:03\n",
            "   -------------------------------- ------ 169.5/203.1 MB 11.7 MB/s eta 0:00:03\n",
            "   -------------------------------- ------ 170.0/203.1 MB 11.7 MB/s eta 0:00:03\n",
            "   -------------------------------- ------ 170.5/203.1 MB 11.7 MB/s eta 0:00:03\n",
            "   -------------------------------- ------ 171.1/203.1 MB 11.7 MB/s eta 0:00:03\n",
            "   -------------------------------- ------ 171.6/203.1 MB 11.7 MB/s eta 0:00:03\n",
            "   --------------------------------- ----- 172.2/203.1 MB 11.7 MB/s eta 0:00:03\n",
            "   --------------------------------- ----- 172.7/203.1 MB 11.9 MB/s eta 0:00:03\n",
            "   --------------------------------- ----- 173.2/203.1 MB 11.7 MB/s eta 0:00:03\n",
            "   --------------------------------- ----- 173.8/203.1 MB 11.7 MB/s eta 0:00:03\n",
            "   --------------------------------- ----- 174.3/203.1 MB 11.7 MB/s eta 0:00:03\n",
            "   --------------------------------- ----- 174.8/203.1 MB 11.7 MB/s eta 0:00:03\n",
            "   --------------------------------- ----- 175.3/203.1 MB 11.5 MB/s eta 0:00:03\n",
            "   --------------------------------- ----- 175.8/203.1 MB 11.5 MB/s eta 0:00:03\n",
            "   --------------------------------- ----- 176.3/203.1 MB 11.5 MB/s eta 0:00:03\n",
            "   --------------------------------- ----- 176.8/203.1 MB 11.5 MB/s eta 0:00:03\n",
            "   ---------------------------------- ---- 177.4/203.1 MB 11.5 MB/s eta 0:00:03\n",
            "   ---------------------------------- ---- 177.9/203.1 MB 11.7 MB/s eta 0:00:03\n",
            "   ---------------------------------- ---- 178.5/203.1 MB 11.5 MB/s eta 0:00:03\n",
            "   ---------------------------------- ---- 179.0/203.1 MB 11.7 MB/s eta 0:00:03\n",
            "   ---------------------------------- ---- 179.5/203.1 MB 11.5 MB/s eta 0:00:03\n",
            "   ---------------------------------- ---- 180.1/203.1 MB 11.7 MB/s eta 0:00:02\n",
            "   ---------------------------------- ---- 180.6/203.1 MB 11.7 MB/s eta 0:00:02\n",
            "   ---------------------------------- ---- 181.2/203.1 MB 11.7 MB/s eta 0:00:02\n",
            "   ---------------------------------- ---- 181.7/203.1 MB 11.7 MB/s eta 0:00:02\n",
            "   ----------------------------------- --- 182.3/203.1 MB 11.7 MB/s eta 0:00:02\n",
            "   ----------------------------------- --- 182.8/203.1 MB 11.5 MB/s eta 0:00:02\n",
            "   ----------------------------------- --- 183.3/203.1 MB 11.5 MB/s eta 0:00:02\n",
            "   ----------------------------------- --- 183.9/203.1 MB 11.5 MB/s eta 0:00:02\n",
            "   ----------------------------------- --- 184.5/203.1 MB 11.5 MB/s eta 0:00:02\n",
            "   ----------------------------------- --- 185.0/203.1 MB 11.7 MB/s eta 0:00:02\n",
            "   ----------------------------------- --- 185.5/203.1 MB 11.7 MB/s eta 0:00:02\n",
            "   ----------------------------------- --- 186.1/203.1 MB 11.7 MB/s eta 0:00:02\n",
            "   ----------------------------------- --- 186.6/203.1 MB 11.7 MB/s eta 0:00:02\n",
            "   ----------------------------------- --- 187.2/203.1 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------------------------------ -- 187.7/203.1 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------------------------------ -- 188.2/203.1 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------------------------------ -- 188.7/203.1 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------------------------ -- 189.2/203.1 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------------------------ -- 189.8/203.1 MB 11.9 MB/s eta 0:00:02\n",
            "   ------------------------------------ -- 190.3/203.1 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------------------------------ -- 190.9/203.1 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------------------------------ -- 191.4/203.1 MB 11.7 MB/s eta 0:00:02\n",
            "   ------------------------------------ -- 191.9/203.1 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ -- 192.5/203.1 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 193.0/203.1 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 193.5/203.1 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 194.0/203.1 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 194.5/203.1 MB 11.5 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 195.1/203.1 MB 11.5 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 195.6/203.1 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 196.1/203.1 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 196.7/203.1 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 197.1/203.1 MB 11.5 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 197.6/203.1 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  198.1/203.1 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  198.7/203.1 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  199.2/203.1 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  199.8/203.1 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  200.3/203.1 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  200.8/203.1 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  201.3/203.1 MB 11.3 MB/s eta 0:00:01\n",
            "   --------------------------------------  201.8/203.1 MB 11.3 MB/s eta 0:00:01\n",
            "   --------------------------------------  202.2/203.1 MB 11.3 MB/s eta 0:00:01\n",
            "   --------------------------------------  202.8/203.1 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 11.1 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 11.1 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 11.1 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 11.1 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 11.1 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 11.1 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 11.1 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 11.1 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 11.1 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 11.1 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 11.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 203.1/203.1 MB 7.1 MB/s eta 0:00:00\n",
            "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.5/6.2 MB 16.5 MB/s eta 0:00:01\n",
            "   ------ --------------------------------- 1.0/6.2 MB 12.6 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 1.5/6.2 MB 12.0 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 2.0/6.2 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 2.5/6.2 MB 11.4 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 3.0/6.2 MB 11.4 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 3.6/6.2 MB 11.5 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 4.1/6.2 MB 11.5 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 4.6/6.2 MB 11.3 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 5.2/6.2 MB 11.3 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 5.7/6.2 MB 11.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  6.2/6.2 MB 11.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 6.2/6.2 MB 10.7 MB/s eta 0:00:00\n",
            "Downloading google_auth-2.17.0-py2.py3-none-any.whl (178 kB)\n",
            "   ---------------------------------------- 0.0/178.1 kB ? eta -:--:--\n",
            "   --------------------------------------  174.1/178.1 kB 10.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 178.1/178.1 kB 3.6 MB/s eta 0:00:00\n",
            "Downloading httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
            "   ---------------------------------------- 0.0/96.6 kB ? eta -:--:--\n",
            "   ---------------------------------------- 96.6/96.6 kB 5.4 MB/s eta 0:00:00\n",
            "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
            "   ---------------------------------------- 0.0/78.4 kB ? eta -:--:--\n",
            "   ---------------------------------------- 78.4/78.4 kB 4.3 MB/s eta 0:00:00\n",
            "Downloading argcomplete-3.5.1-py3-none-any.whl (43 kB)\n",
            "   ---------------------------------------- 0.0/43.5 kB ? eta -:--:--\n",
            "   ---------------------------------------- 43.5/43.5 kB 2.1 MB/s eta 0:00:00\n",
            "Downloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
            "Downloading cvxpy-1.5.3-cp312-cp312-win_amd64.whl (1.1 MB)\n",
            "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
            "   ------------------- -------------------- 0.5/1.1 MB 10.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  1.0/1.1 MB 13.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.1/1.1 MB 9.6 MB/s eta 0:00:00\n",
            "Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Downloading google_apitools-0.5.32-py3-none-any.whl (135 kB)\n",
            "   ---------------------------------------- 0.0/135.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 135.7/135.7 kB 8.4 MB/s eta 0:00:00\n",
            "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Downloading google_reauth-0.1.1-py2.py3-none-any.whl (17 kB)\n",
            "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading pyOpenSSL-24.2.1-py3-none-any.whl (58 kB)\n",
            "   ---------------------------------------- 0.0/58.4 kB ? eta -:--:--\n",
            "   ---------------------------------------- 58.4/58.4 kB 3.0 MB/s eta 0:00:00\n",
            "Downloading aiohttp-3.10.10-cp312-cp312-win_amd64.whl (379 kB)\n",
            "   ---------------------------------------- 0.0/379.3 kB ? eta -:--:--\n",
            "   --------------------------------------  378.9/379.3 kB 11.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 379.3/379.3 kB 7.9 MB/s eta 0:00:00\n",
            "Downloading boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
            "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
            "   ------------- -------------------------- 0.5/1.4 MB 14.2 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 0.9/1.4 MB 11.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------  1.3/1.4 MB 10.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.4/1.4 MB 8.6 MB/s eta 0:00:00\n",
            "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
            "Downloading clarabel-0.9.0-cp37-abi3-win_amd64.whl (736 kB)\n",
            "   ---------------------------------------- 0.0/736.4 kB ? eta -:--:--\n",
            "   -------------------------- ------------ 501.8/736.4 kB 10.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 736.4/736.4 kB 7.7 MB/s eta 0:00:00\n",
            "Downloading cryptography-43.0.1-cp39-abi3-win_amd64.whl (3.1 MB)\n",
            "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 0.4/3.1 MB 13.4 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 0.9/3.1 MB 9.9 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 1.5/3.1 MB 11.6 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 2.0/3.1 MB 10.5 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 2.4/3.1 MB 11.1 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 2.9/3.1 MB 11.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 3.1/3.1 MB 10.3 MB/s eta 0:00:00\n",
            "Downloading ecos-2.0.14-cp312-cp312-win_amd64.whl (72 kB)\n",
            "   ---------------------------------------- 0.0/72.2 kB ? eta -:--:--\n",
            "   ---------------------------------------- 72.2/72.2 kB 3.9 MB/s eta 0:00:00\n",
            "Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
            "   ---------------------------------------- 0.0/98.2 kB ? eta -:--:--\n",
            "   ---------------------------------------- 98.2/98.2 kB 5.5 MB/s eta 0:00:00\n",
            "Downloading osqp-0.6.7.post3-cp312-cp312-win_amd64.whl (293 kB)\n",
            "   ---------------------------------------- 0.0/293.2 kB ? eta -:--:--\n",
            "   ---------------------------------------- 293.2/293.2 kB 9.1 MB/s eta 0:00:00\n",
            "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
            "   ---------------------------------------- 0.0/181.5 kB ? eta -:--:--\n",
            "   ---------------------------------------- 181.5/181.5 kB 5.5 MB/s eta 0:00:00\n",
            "Downloading scs-3.2.7-cp312-cp312-win_amd64.whl (8.4 MB)\n",
            "   ---------------------------------------- 0.0/8.4 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.5/8.4 MB 14.9 MB/s eta 0:00:01\n",
            "   ---- ----------------------------------- 1.0/8.4 MB 12.9 MB/s eta 0:00:01\n",
            "   ------- -------------------------------- 1.6/8.4 MB 12.5 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 2.1/8.4 MB 12.2 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 2.7/8.4 MB 12.3 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 3.2/8.4 MB 11.9 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 3.7/8.4 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 4.1/8.4 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 4.7/8.4 MB 11.5 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 5.2/8.4 MB 11.9 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 5.7/8.4 MB 11.7 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 6.3/8.4 MB 11.8 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 6.7/8.4 MB 11.7 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 7.3/8.4 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 7.8/8.4 MB 11.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  8.4/8.4 MB 11.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  8.4/8.4 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 8.4/8.4 MB 10.8 MB/s eta 0:00:00\n",
            "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
            "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
            "   ---------------------------------------- 0.0/63.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 63.0/63.0 kB 3.3 MB/s eta 0:00:00\n",
            "Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
            "   ---------------------------------------- 0.0/182.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 182.0/182.0 kB 5.5 MB/s eta 0:00:00\n",
            "Downloading frozenlist-1.4.1-cp312-cp312-win_amd64.whl (50 kB)\n",
            "   ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
            "   ---------------------------------------- 50.5/50.5 kB 1.3 MB/s eta 0:00:00\n",
            "Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "   ---------------------------------------- 0.0/83.1 kB ? eta -:--:--\n",
            "   ---------------------------------------- 83.1/83.1 kB 2.4 MB/s eta 0:00:00\n",
            "Downloading yarl-1.15.4-cp312-cp312-win_amd64.whl (84 kB)\n",
            "   ---------------------------------------- 0.0/84.9 kB ? eta -:--:--\n",
            "   ---------------------------------------- 84.9/84.9 kB 4.7 MB/s eta 0:00:00\n",
            "Downloading qdldl-0.1.7.post4-cp312-cp312-win_amd64.whl (87 kB)\n",
            "   ---------------------------------------- 0.0/87.8 kB ? eta -:--:--\n",
            "   ---------------------------------------- 87.8/87.8 kB 4.8 MB/s eta 0:00:00\n",
            "Downloading propcache-0.2.0-cp312-cp312-win_amd64.whl (44 kB)\n",
            "   ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
            "   ---------------------------------------- 44.4/44.4 kB 1.1 MB/s eta 0:00:00\n",
            "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
            "   ---------------------------------------- 0.0/117.6 kB ? eta -:--:--\n",
            "   ---------------------------------------- 117.6/117.6 kB 3.5 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: learn2learn, gym, qpth, gsutil, crcmod, gcs-oauth2-boto-plugin, retry_decorator, pyu2f\n",
            "  Building wheel for learn2learn (setup.py): started\n",
            "  Building wheel for learn2learn (setup.py): finished with status 'error'\n",
            "  Running setup.py clean for learn2learn\n",
            "  Building wheel for gym (pyproject.toml): started\n",
            "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827637 sha256=c5ac5a8369fb2b094da685c995e807df95f75e81de920317d0488a23bd76ef8e\n",
            "  Stored in directory: c:\\users\\abhishek p\\appdata\\local\\pip\\cache\\wheels\\95\\51\\6c\\9bb05ebbe7c5cb8171dfaa3611f32622ca4658d53f31c79077\n",
            "  Building wheel for qpth (setup.py): started\n",
            "  Building wheel for qpth (setup.py): finished with status 'done'\n",
            "  Created wheel for qpth: filename=qpth-0.0.18-py3-none-any.whl size=19555 sha256=7e1d679809c1a5b4e058922f4a640ed7af6a798633072ee2ef7d35dde442e952\n",
            "  Stored in directory: c:\\users\\abhishek p\\appdata\\local\\pip\\cache\\wheels\\a7\\92\\3c\\eef342f44096c656e7adcb4b49f3bc121aba11c3ce6d1d6ebf\n",
            "  Building wheel for gsutil (setup.py): started\n",
            "  Building wheel for gsutil (setup.py): finished with status 'done'\n",
            "  Created wheel for gsutil: filename=gsutil-5.31-py3-none-any.whl size=3789458 sha256=0122fabbd36cf8f3aa8ba359c581fef8b3f937e1339dc34253cb2e6a3afe335f\n",
            "  Stored in directory: c:\\users\\abhishek p\\appdata\\local\\pip\\cache\\wheels\\51\\ab\\64\\62a6176c68f05eb0ef9cab56918e38946057992d2b0837f7b1\n",
            "  Building wheel for crcmod (setup.py): started\n",
            "  Building wheel for crcmod (setup.py): finished with status 'done'\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-py3-none-any.whl size=18869 sha256=83534c60d9281ee58b2c4be74fa9ce48f84ffbd1f39927124d92242f12a0d870\n",
            "  Stored in directory: c:\\users\\abhishek p\\appdata\\local\\pip\\cache\\wheels\\76\\08\\0b\\caa8b1380122cbfe6a03eaccbec0f63c67e619af4e30ca5e2a\n",
            "  Building wheel for gcs-oauth2-boto-plugin (setup.py): started\n",
            "  Building wheel for gcs-oauth2-boto-plugin (setup.py): finished with status 'done'\n",
            "  Created wheel for gcs-oauth2-boto-plugin: filename=gcs_oauth2_boto_plugin-3.2-py3-none-any.whl size=24481 sha256=86ef7d31d66b755a916313bd83b268233b1cb3f2ab34740e283610639f51cc1f\n",
            "  Stored in directory: c:\\users\\abhishek p\\appdata\\local\\pip\\cache\\wheels\\66\\f6\\ee\\0c8bdf52a3cf333943dcf0cf44ccdd3095a8d06209d4e40b2e\n",
            "  Building wheel for retry_decorator (setup.py): started\n",
            "  Building wheel for retry_decorator (setup.py): finished with status 'done'\n",
            "  Created wheel for retry_decorator: filename=retry_decorator-1.1.1-py2.py3-none-any.whl size=3652 sha256=f9774bae031cbf455b9d8c544cd395e4e1e8cfde1bc1d205f2b42bdf23f6938d\n",
            "  Stored in directory: c:\\users\\abhishek p\\appdata\\local\\pip\\cache\\wheels\\47\\d8\\ff\\f25b34f7d273518501f5926d0dd2565fddff96a49c8da82122\n",
            "  Building wheel for pyu2f (setup.py): started\n",
            "  Building wheel for pyu2f (setup.py): finished with status 'done'\n",
            "  Created wheel for pyu2f: filename=pyu2f-0.1.5-py3-none-any.whl size=39413 sha256=5fdc051b0cf574ee12b6ca625f396577753d27fdb6020d40efbfe5867905caa5\n",
            "  Stored in directory: c:\\users\\abhishek p\\appdata\\local\\pip\\cache\\wheels\\c8\\65\\96\\5db9bc4a34b1f0200f3cfaf6e675effad84f4d7b167e548c34\n",
            "Successfully built gym qpth gsutil crcmod gcs-oauth2-boto-plugin retry_decorator pyu2f\n",
            "Failed to build learn2learn\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py bdist_wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [163 lines of output]\n",
            "      c:\\Python312\\Lib\\site-packages\\setuptools\\__init__.py:80: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
            "      !!\n",
            "      \n",
            "              ********************************************************************************\n",
            "              Requirements should be satisfied by a PEP 517 installer.\n",
            "              If you are using pip, you can try `pip install --use-pep517`.\n",
            "              ********************************************************************************\n",
            "      \n",
            "      !!\n",
            "        dist.fetch_build_eggs(dist.setup_requires)\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-312\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\n",
            "      copying learn2learn\\_version.py -> build\\lib.win-amd64-cpython-312\\learn2learn\n",
            "      copying learn2learn\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\n",
            "      creating build\\lib.win-amd64-cpython-312\\tests\n",
            "      copying tests\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
            "      copying learn2learn\\algorithms\\base_learner.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
            "      copying learn2learn\\algorithms\\gbml.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
            "      copying learn2learn\\algorithms\\maml.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
            "      copying learn2learn\\algorithms\\meta_sgd.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
            "      copying learn2learn\\algorithms\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\data\n",
            "      copying learn2learn\\data\\samplers.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\data\n",
            "      copying learn2learn\\data\\utils.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\data\n",
            "      copying learn2learn\\data\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\data\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\gym\n",
            "      copying learn2learn\\gym\\async_vec_env.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\n",
            "      copying learn2learn\\gym\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
            "      copying learn2learn\\nn\\kroneckers.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
            "      copying learn2learn\\nn\\metaoptnet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
            "      copying learn2learn\\nn\\misc.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
            "      copying learn2learn\\nn\\protonet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
            "      copying learn2learn\\nn\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\optim\n",
            "      copying learn2learn\\optim\\learnable_optimizer.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\n",
            "      copying learn2learn\\optim\\parameter_update.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\n",
            "      copying learn2learn\\optim\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\text\n",
            "      copying learn2learn\\text\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\text\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\utils\n",
            "      copying learn2learn\\utils\\lightning.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\utils\n",
            "      copying learn2learn\\utils\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\utils\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\vision\n",
            "      copying learn2learn\\vision\\transforms.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\n",
            "      copying learn2learn\\vision\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
            "      copying learn2learn\\algorithms\\lightning\\lightning_anil.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
            "      copying learn2learn\\algorithms\\lightning\\lightning_episodic_module.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
            "      copying learn2learn\\algorithms\\lightning\\lightning_maml.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
            "      copying learn2learn\\algorithms\\lightning\\lightning_metaoptnet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
            "      copying learn2learn\\algorithms\\lightning\\lightning_protonet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
            "      copying learn2learn\\algorithms\\lightning\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\n",
            "      copying learn2learn\\gym\\envs\\meta_env.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\n",
            "      copying learn2learn\\gym\\envs\\subproc_vec_env.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\n",
            "      copying learn2learn\\gym\\envs\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\metaworld\n",
            "      copying learn2learn\\gym\\envs\\metaworld\\metaworld.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\metaworld\n",
            "      copying learn2learn\\gym\\envs\\metaworld\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\metaworld\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
            "      copying learn2learn\\gym\\envs\\mujoco\\ant_direction.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
            "      copying learn2learn\\gym\\envs\\mujoco\\ant_forward_backward.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
            "      copying learn2learn\\gym\\envs\\mujoco\\dummy_mujoco_env.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
            "      copying learn2learn\\gym\\envs\\mujoco\\halfcheetah_forward_backward.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
            "      copying learn2learn\\gym\\envs\\mujoco\\humanoid_direction.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
            "      copying learn2learn\\gym\\envs\\mujoco\\humanoid_forward_backward.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
            "      copying learn2learn\\gym\\envs\\mujoco\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\particles\n",
            "      copying learn2learn\\gym\\envs\\particles\\particles_2d.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\particles\n",
            "      copying learn2learn\\gym\\envs\\particles\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\particles\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\nn\\metalayers\n",
            "      copying learn2learn\\nn\\metalayers\\metamodule.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\\metalayers\n",
            "      copying learn2learn\\nn\\metalayers\\parameter_transform.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\\metalayers\n",
            "      copying learn2learn\\nn\\metalayers\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\\metalayers\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
            "      copying learn2learn\\optim\\transforms\\kronecker_transform.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
            "      copying learn2learn\\optim\\transforms\\metacurvature_transform.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
            "      copying learn2learn\\optim\\transforms\\module_transform.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
            "      copying learn2learn\\optim\\transforms\\transform_dictionary.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
            "      copying learn2learn\\optim\\transforms\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\update_rules\n",
            "      copying learn2learn\\optim\\update_rules\\differentiable_sgd.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\update_rules\n",
            "      copying learn2learn\\optim\\update_rules\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\update_rules\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\text\\datasets\n",
            "      copying learn2learn\\text\\datasets\\news_classification.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\text\\datasets\n",
            "      copying learn2learn\\text\\datasets\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\text\\datasets\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
            "      copying learn2learn\\vision\\benchmarks\\cifarfs_benchmark.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
            "      copying learn2learn\\vision\\benchmarks\\fc100_benchmark.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
            "      copying learn2learn\\vision\\benchmarks\\mini_imagenet_benchmark.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
            "      copying learn2learn\\vision\\benchmarks\\omniglot_benchmark.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
            "      copying learn2learn\\vision\\benchmarks\\tiered_imagenet_benchmark.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
            "      copying learn2learn\\vision\\benchmarks\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\cifarfs.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\cu_birds200.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\describable_textures.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\fc100.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\fgvc_aircraft.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\fgvc_fungi.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\full_omniglot.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\mini_imagenet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\quickdraw.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\tiered_imagenet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\vgg_flowers.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\models\n",
            "      copying learn2learn\\vision\\models\\cnn4.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\models\n",
            "      copying learn2learn\\vision\\models\\resnet12.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\models\n",
            "      copying learn2learn\\vision\\models\\wrn28.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\models\n",
            "      copying learn2learn\\vision\\models\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\models\n",
            "      creating build\\lib.win-amd64-cpython-312\\tests\\integration\n",
            "      copying tests\\integration\\maml_miniimagenet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\integration\n",
            "      copying tests\\integration\\maml_omniglot_test.py -> build\\lib.win-amd64-cpython-312\\tests\\integration\n",
            "      copying tests\\integration\\protonets_miniimagenet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\integration\n",
            "      copying tests\\integration\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\integration\n",
            "      creating build\\lib.win-amd64-cpython-312\\tests\\unit\n",
            "      copying tests\\unit\\utils_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\n",
            "      copying tests\\unit\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\n",
            "      creating build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      copying tests\\unit\\algorithms\\gbml_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      copying tests\\unit\\algorithms\\lightning_anil_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      copying tests\\unit\\algorithms\\lightning_maml_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      copying tests\\unit\\algorithms\\lightning_metaoptnet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      copying tests\\unit\\algorithms\\lightning_protonet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      copying tests\\unit\\algorithms\\maml_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      copying tests\\unit\\algorithms\\metasgd_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      copying tests\\unit\\algorithms\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      creating build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
            "      copying tests\\unit\\data\\metadataset_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
            "      copying tests\\unit\\data\\task_dataset_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
            "      copying tests\\unit\\data\\transforms_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
            "      copying tests\\unit\\data\\utils_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
            "      copying tests\\unit\\data\\util_datasets.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
            "      copying tests\\unit\\data\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
            "      creating build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
            "      copying tests\\unit\\nn\\kroneckers_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
            "      copying tests\\unit\\nn\\metaoptnet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
            "      copying tests\\unit\\nn\\misc.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
            "      copying tests\\unit\\nn\\protonet_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
            "      copying tests\\unit\\nn\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
            "      creating build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\benchmarks_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\cifarfs_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\cu_birds200_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\describable_textures_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\fc100_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\fgvc_aircraft_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\pretrained_backbones_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\quickdraw_test_no.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\tiered_imagenet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\transform_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\vgg_flowers_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      running build_ext\n",
            "      building 'learn2learn.data.meta_dataset' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for learn2learn\n",
            "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (learn2learn)\n",
            "\n",
            "[notice] A new release of pip is available: 24.1 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install learn2learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting learn2learn\n",
            "  Using cached learn2learn-0.2.0.tar.gz (7.0 MB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: numpy>=1.15.4 in c:\\python312\\lib\\site-packages (from learn2learn) (1.26.4)\n",
            "Collecting gym>=0.14.0 (from learn2learn)\n",
            "  Using cached gym-0.26.2-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.1.0 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from learn2learn) (2.5.0)\n",
            "Requirement already satisfied: torchvision>=0.3.0 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from learn2learn) (0.20.0)\n",
            "Requirement already satisfied: scipy in c:\\python312\\lib\\site-packages (from learn2learn) (1.13.1)\n",
            "Requirement already satisfied: requests in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from learn2learn) (2.32.3)\n",
            "Collecting gsutil (from learn2learn)\n",
            "  Using cached gsutil-5.31-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from learn2learn) (4.66.5)\n",
            "Collecting qpth>=0.0.15 (from learn2learn)\n",
            "  Using cached qpth-0.0.18-py3-none-any.whl\n",
            "Collecting cloudpickle>=1.2.0 (from gym>=0.14.0->learn2learn)\n",
            "  Using cached cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting gym-notices>=0.0.4 (from gym>=0.14.0->learn2learn)\n",
            "  Using cached gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting cvxpy>=1.1.0 (from qpth>=0.0.15->learn2learn)\n",
            "  Using cached cvxpy-1.5.3-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.1.0->learn2learn) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python312\\lib\\site-packages (from torch>=1.1.0->learn2learn) (4.12.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.1.0->learn2learn) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.1.0->learn2learn) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.1.0->learn2learn) (2024.6.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.1.0->learn2learn) (75.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.1.0->learn2learn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=1.1.0->learn2learn) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\python312\\lib\\site-packages (from torchvision>=0.3.0->learn2learn) (10.3.0)\n",
            "Collecting argcomplete>=1.9.4 (from gsutil->learn2learn)\n",
            "  Using cached argcomplete-3.5.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting crcmod>=1.7 (from gsutil->learn2learn)\n",
            "  Using cached crcmod-1.7-py3-none-any.whl\n",
            "Collecting fasteners>=0.14.1 (from gsutil->learn2learn)\n",
            "  Using cached fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting gcs-oauth2-boto-plugin>=3.2 (from gsutil->learn2learn)\n",
            "  Using cached gcs_oauth2_boto_plugin-3.2-py3-none-any.whl\n",
            "Collecting google-apitools>=0.5.32 (from gsutil->learn2learn)\n",
            "  Using cached google_apitools-0.5.32-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting httplib2==0.20.4 (from gsutil->learn2learn)\n",
            "  Using cached httplib2-0.20.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting google-reauth>=0.1.0 (from gsutil->learn2learn)\n",
            "  Using cached google_reauth-0.1.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting monotonic>=1.4 (from gsutil->learn2learn)\n",
            "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyOpenSSL>=0.13 (from gsutil->learn2learn)\n",
            "  Using cached pyOpenSSL-24.2.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting retry-decorator>=1.0.0 (from gsutil->learn2learn)\n",
            "  Using cached retry_decorator-1.1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.16.0 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from gsutil->learn2learn) (1.16.0)\n",
            "Collecting google-auth==2.17.0 (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Using cached google_auth-2.17.0-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting google-auth-httplib2>=0.2.0 (from gsutil->learn2learn)\n",
            "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0dev,>=3.6.2 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (3.10.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\python312\\lib\\site-packages (from httplib2==0.20.4->gsutil->learn2learn) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from requests->learn2learn) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests->learn2learn) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests->learn2learn) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from requests->learn2learn) (2024.6.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->learn2learn) (0.4.6)\n",
            "Collecting osqp>=0.6.2 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn)\n",
            "  Using cached osqp-0.6.7.post3-cp312-cp312-win_amd64.whl.metadata (2.0 kB)\n",
            "Collecting ecos>=2 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn)\n",
            "  Using cached ecos-2.0.14-cp312-cp312-win_amd64.whl.metadata (8.2 kB)\n",
            "Collecting clarabel>=0.5.0 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn)\n",
            "  Using cached clarabel-0.9.0-cp37-abi3-win_amd64.whl.metadata (4.8 kB)\n",
            "Collecting scs>=3.2.4.post1 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn)\n",
            "  Using cached scs-3.2.7-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Using cached rsa-4.7.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting boto>=2.29.1 (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn)\n",
            "  Using cached boto-2.49.0-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting oauth2client>=2.2.0 (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn)\n",
            "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn)\n",
            "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pyu2f (from google-reauth>=0.1.0->gsutil->learn2learn)\n",
            "  Using cached pyu2f-0.1.5-py3-none-any.whl\n",
            "Collecting cryptography<44,>=41.0.5 (from pyOpenSSL>=0.13->gsutil->learn2learn)\n",
            "  Using cached cryptography-43.0.1-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch>=1.1.0->learn2learn) (2.1.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (1.15.4)\n",
            "Collecting cffi>=1.12 (from cryptography<44,>=41.0.5->pyOpenSSL>=0.13->gsutil->learn2learn)\n",
            "  Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
            "Collecting qdldl (from osqp>=0.6.2->cvxpy>=1.1.0->qpth>=0.0.15->learn2learn)\n",
            "  Using cached qdldl-0.1.7.post4-cp312-cp312-win_amd64.whl.metadata (1.8 kB)\n",
            "Collecting pycparser (from cffi>=1.12->cryptography<44,>=41.0.5->pyOpenSSL>=0.13->gsutil->learn2learn)\n",
            "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn) (0.2.0)\n",
            "Using cached google_auth-2.17.0-py2.py3-none-any.whl (178 kB)\n",
            "Using cached httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
            "Using cached argcomplete-3.5.1-py3-none-any.whl (43 kB)\n",
            "Using cached cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
            "Using cached cvxpy-1.5.3-cp312-cp312-win_amd64.whl (1.1 MB)\n",
            "Using cached fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Using cached google_apitools-0.5.32-py3-none-any.whl (135 kB)\n",
            "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Using cached google_reauth-0.1.1-py2.py3-none-any.whl (17 kB)\n",
            "Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
            "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Using cached pyOpenSSL-24.2.1-py3-none-any.whl (58 kB)\n",
            "Using cached boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
            "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
            "Using cached clarabel-0.9.0-cp37-abi3-win_amd64.whl (736 kB)\n",
            "Using cached cryptography-43.0.1-cp39-abi3-win_amd64.whl (3.1 MB)\n",
            "Using cached ecos-2.0.14-cp312-cp312-win_amd64.whl (72 kB)\n",
            "Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
            "Using cached osqp-0.6.7.post3-cp312-cp312-win_amd64.whl (293 kB)\n",
            "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
            "Using cached scs-3.2.7-cp312-cp312-win_amd64.whl (8.4 MB)\n",
            "Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
            "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Using cached qdldl-0.1.7.post4-cp312-cp312-win_amd64.whl (87 kB)\n",
            "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
            "Building wheels for collected packages: learn2learn\n",
            "  Building wheel for learn2learn (setup.py): started\n",
            "  Building wheel for learn2learn (setup.py): finished with status 'error'\n",
            "  Running setup.py clean for learn2learn\n",
            "Failed to build learn2learn\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py bdist_wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [161 lines of output]\n",
            "      C:\\Users\\Abhishek P\\AppData\\Roaming\\Python\\Python312\\site-packages\\setuptools\\__init__.py:94: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
            "      !!\n",
            "      \n",
            "              ********************************************************************************\n",
            "              Requirements should be satisfied by a PEP 517 installer.\n",
            "              If you are using pip, you can try `pip install --use-pep517`.\n",
            "              ********************************************************************************\n",
            "      \n",
            "      !!\n",
            "        dist.fetch_build_eggs(dist.setup_requires)\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\n",
            "      copying learn2learn\\_version.py -> build\\lib.win-amd64-cpython-312\\learn2learn\n",
            "      copying learn2learn\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\n",
            "      creating build\\lib.win-amd64-cpython-312\\tests\n",
            "      copying tests\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
            "      copying learn2learn\\algorithms\\base_learner.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
            "      copying learn2learn\\algorithms\\gbml.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
            "      copying learn2learn\\algorithms\\maml.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
            "      copying learn2learn\\algorithms\\meta_sgd.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
            "      copying learn2learn\\algorithms\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\data\n",
            "      copying learn2learn\\data\\samplers.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\data\n",
            "      copying learn2learn\\data\\utils.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\data\n",
            "      copying learn2learn\\data\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\data\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\gym\n",
            "      copying learn2learn\\gym\\async_vec_env.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\n",
            "      copying learn2learn\\gym\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
            "      copying learn2learn\\nn\\kroneckers.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
            "      copying learn2learn\\nn\\metaoptnet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
            "      copying learn2learn\\nn\\misc.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
            "      copying learn2learn\\nn\\protonet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
            "      copying learn2learn\\nn\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\optim\n",
            "      copying learn2learn\\optim\\learnable_optimizer.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\n",
            "      copying learn2learn\\optim\\parameter_update.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\n",
            "      copying learn2learn\\optim\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\text\n",
            "      copying learn2learn\\text\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\text\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\utils\n",
            "      copying learn2learn\\utils\\lightning.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\utils\n",
            "      copying learn2learn\\utils\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\utils\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\vision\n",
            "      copying learn2learn\\vision\\transforms.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\n",
            "      copying learn2learn\\vision\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
            "      copying learn2learn\\algorithms\\lightning\\lightning_anil.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
            "      copying learn2learn\\algorithms\\lightning\\lightning_episodic_module.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
            "      copying learn2learn\\algorithms\\lightning\\lightning_maml.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
            "      copying learn2learn\\algorithms\\lightning\\lightning_metaoptnet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
            "      copying learn2learn\\algorithms\\lightning\\lightning_protonet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
            "      copying learn2learn\\algorithms\\lightning\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\n",
            "      copying learn2learn\\gym\\envs\\meta_env.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\n",
            "      copying learn2learn\\gym\\envs\\subproc_vec_env.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\n",
            "      copying learn2learn\\gym\\envs\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\metaworld\n",
            "      copying learn2learn\\gym\\envs\\metaworld\\metaworld.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\metaworld\n",
            "      copying learn2learn\\gym\\envs\\metaworld\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\metaworld\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
            "      copying learn2learn\\gym\\envs\\mujoco\\ant_direction.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
            "      copying learn2learn\\gym\\envs\\mujoco\\ant_forward_backward.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
            "      copying learn2learn\\gym\\envs\\mujoco\\dummy_mujoco_env.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
            "      copying learn2learn\\gym\\envs\\mujoco\\halfcheetah_forward_backward.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
            "      copying learn2learn\\gym\\envs\\mujoco\\humanoid_direction.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
            "      copying learn2learn\\gym\\envs\\mujoco\\humanoid_forward_backward.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
            "      copying learn2learn\\gym\\envs\\mujoco\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\particles\n",
            "      copying learn2learn\\gym\\envs\\particles\\particles_2d.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\particles\n",
            "      copying learn2learn\\gym\\envs\\particles\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\particles\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\nn\\metalayers\n",
            "      copying learn2learn\\nn\\metalayers\\metamodule.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\\metalayers\n",
            "      copying learn2learn\\nn\\metalayers\\parameter_transform.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\\metalayers\n",
            "      copying learn2learn\\nn\\metalayers\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\\metalayers\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
            "      copying learn2learn\\optim\\transforms\\kronecker_transform.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
            "      copying learn2learn\\optim\\transforms\\metacurvature_transform.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
            "      copying learn2learn\\optim\\transforms\\module_transform.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
            "      copying learn2learn\\optim\\transforms\\transform_dictionary.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
            "      copying learn2learn\\optim\\transforms\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\update_rules\n",
            "      copying learn2learn\\optim\\update_rules\\differentiable_sgd.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\update_rules\n",
            "      copying learn2learn\\optim\\update_rules\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\update_rules\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\text\\datasets\n",
            "      copying learn2learn\\text\\datasets\\news_classification.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\text\\datasets\n",
            "      copying learn2learn\\text\\datasets\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\text\\datasets\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
            "      copying learn2learn\\vision\\benchmarks\\cifarfs_benchmark.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
            "      copying learn2learn\\vision\\benchmarks\\fc100_benchmark.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
            "      copying learn2learn\\vision\\benchmarks\\mini_imagenet_benchmark.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
            "      copying learn2learn\\vision\\benchmarks\\omniglot_benchmark.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
            "      copying learn2learn\\vision\\benchmarks\\tiered_imagenet_benchmark.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
            "      copying learn2learn\\vision\\benchmarks\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\cifarfs.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\cu_birds200.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\describable_textures.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\fc100.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\fgvc_aircraft.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\fgvc_fungi.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\full_omniglot.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\mini_imagenet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\quickdraw.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\tiered_imagenet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\vgg_flowers.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      copying learn2learn\\vision\\datasets\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
            "      creating build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\models\n",
            "      copying learn2learn\\vision\\models\\cnn4.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\models\n",
            "      copying learn2learn\\vision\\models\\resnet12.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\models\n",
            "      copying learn2learn\\vision\\models\\wrn28.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\models\n",
            "      copying learn2learn\\vision\\models\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\models\n",
            "      creating build\\lib.win-amd64-cpython-312\\tests\\integration\n",
            "      copying tests\\integration\\maml_miniimagenet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\integration\n",
            "      copying tests\\integration\\maml_omniglot_test.py -> build\\lib.win-amd64-cpython-312\\tests\\integration\n",
            "      copying tests\\integration\\protonets_miniimagenet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\integration\n",
            "      copying tests\\integration\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\integration\n",
            "      creating build\\lib.win-amd64-cpython-312\\tests\\unit\n",
            "      copying tests\\unit\\utils_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\n",
            "      copying tests\\unit\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\n",
            "      creating build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      copying tests\\unit\\algorithms\\gbml_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      copying tests\\unit\\algorithms\\lightning_anil_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      copying tests\\unit\\algorithms\\lightning_maml_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      copying tests\\unit\\algorithms\\lightning_metaoptnet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      copying tests\\unit\\algorithms\\lightning_protonet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      copying tests\\unit\\algorithms\\maml_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      copying tests\\unit\\algorithms\\metasgd_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      copying tests\\unit\\algorithms\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
            "      creating build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
            "      copying tests\\unit\\data\\metadataset_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
            "      copying tests\\unit\\data\\task_dataset_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
            "      copying tests\\unit\\data\\transforms_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
            "      copying tests\\unit\\data\\utils_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
            "      copying tests\\unit\\data\\util_datasets.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
            "      copying tests\\unit\\data\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
            "      creating build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
            "      copying tests\\unit\\nn\\kroneckers_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
            "      copying tests\\unit\\nn\\metaoptnet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
            "      copying tests\\unit\\nn\\misc.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
            "      copying tests\\unit\\nn\\protonet_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
            "      copying tests\\unit\\nn\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
            "      creating build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\benchmarks_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\cifarfs_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\cu_birds200_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\describable_textures_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\fc100_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\fgvc_aircraft_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\pretrained_backbones_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\quickdraw_test_no.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\tiered_imagenet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\transform_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\vgg_flowers_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      copying tests\\unit\\vision\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
            "      running build_ext\n",
            "      building 'learn2learn.data.meta_dataset' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for learn2learn\n",
            "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (learn2learn)\n"
          ]
        }
      ],
      "source": [
        "pip install --user learn2learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (24.2)\n",
            "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (70.1.0)\n",
            "Collecting setuptools\n",
            "  Using cached setuptools-75.2.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: wheel in c:\\python312\\lib\\site-packages (0.43.0)\n",
            "Collecting wheel\n",
            "  Downloading wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Using cached setuptools-75.2.0-py3-none-any.whl (1.2 MB)\n",
            "Downloading wheel-0.44.0-py3-none-any.whl (67 kB)\n",
            "Installing collected packages: wheel, setuptools\n",
            "Successfully installed setuptools-75.2.0 wheel-0.44.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "   !pip install --user --upgrade pip setuptools wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YS0TxK5mPkcu"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'learn2learn'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlearn2learn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MAML\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'learn2learn'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from learn2learn.algorithms import MAML\n",
        "from torch.optim import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load and Preprocess the Dataset\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\")\n",
        "data = data.dropna()  # Handle missing values\n",
        "X = data.drop(\"median_house_value\", axis=1)\n",
        "y = data[\"median_house_value\"]\n",
        "\n",
        "# Binning the target into 5 classes\n",
        "y_bins = pd.qcut(y, q=5, labels=False)\n",
        "X = pd.get_dummies(X)  # One-hot encode categorical variables\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_bins, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define the Few-Shot Task Loader\n",
        "class TaskLoader:\n",
        "    def __init__(self, X, y, n_classes=5, n_samples=5):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.n_classes = n_classes\n",
        "        self.n_samples = n_samples\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _ in range(10):  # Create 10 tasks\n",
        "            tasks_X, tasks_y = [], []\n",
        "            for class_id in range(self.n_classes):\n",
        "                # Check if there are enough samples for the class\n",
        "                if len(self.X[self.y == class_id]) < self.n_samples:\n",
        "                    raise ValueError(f\"Not enough samples for class {class_id}. Needed {self.n_samples}.\")\n",
        "\n",
        "                class_samples = self.X.loc[self.y == class_id].sample(n=self.n_samples)\n",
        "                tasks_X.append(class_samples)\n",
        "                tasks_y.append(np.full((self.n_samples,), class_id))\n",
        "            yield pd.concat(tasks_X), np.concatenate(tasks_y)\n",
        "\n",
        "# Step 4: Define the Model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(X_train.shape[1], 5)  # 5 classes based on binning\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Step 5: Training Loop with MAML\n",
        "model = SimpleModel()\n",
        "maml = MAML(model, lr=0.01, first_order=True)\n",
        "optimizer = Adam(maml.parameters(), lr=0.001)\n",
        "\n",
        "task_loader = TaskLoader(X_train, y_train)\n",
        "\n",
        "# Loop through each task\n",
        "for task_X, task_y in task_loader:\n",
        "    learner = maml.clone()  # Create a task-specific copy of the model\n",
        "\n",
        "    # Convert DataFrame to NumPy arrays and ensure all data is numeric\n",
        "    task_X_tensor = torch.tensor(task_X.values.astype(np.float32), dtype=torch.float32)\n",
        "    task_y_tensor = torch.tensor(task_y, dtype=torch.long)\n",
        "\n",
        "    task_dataset = TensorDataset(task_X_tensor, task_y_tensor)\n",
        "    task_data_loader = DataLoader(task_dataset, batch_size=2)\n",
        "\n",
        "    # Loop through batches of data\n",
        "    for batch_X, batch_y in task_data_loader:\n",
        "        pred = learner(batch_X)  # Get predictions\n",
        "        loss = nn.CrossEntropyLoss()(pred, batch_y)  # Calculate loss\n",
        "        learner.adapt(loss)  # Adapt to the task\n",
        "\n",
        "    optimizer.step()  # Update the meta-learner\n",
        "\n",
        "# After this point, you can evaluate the learner on unseen tasks or continue training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voZwZNDXBUjo"
      },
      "source": [
        "Multi-Modal Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNxZQ554-wQj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from learn2learn.algorithms import MAML\n",
        "from torch.optim import Adam\n",
        "import gc  # For garbage collection\n",
        "\n",
        "# Load and Preprocess the Dataset (keep this section as is if you're using it)\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\")\n",
        "data = data.dropna()\n",
        "X = data.drop(\"median_house_value\", axis=1)\n",
        "y = data[\"median_house_value\"]\n",
        "y_bins = pd.qcut(y, q=5, labels=False)\n",
        "X = pd.get_dummies(X)\n",
        "\n",
        "# Subset the data if necessary for RAM efficiency (e.g., 10% of the dataset)\n",
        "X = X.sample(frac=0.1, random_state=42)\n",
        "y_bins = y_bins.loc[X.index]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_bins, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Few-Shot Task Loader\n",
        "class TaskLoader:\n",
        "    def __init__(self, X, y, n_classes=5, n_samples=5):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.n_classes = n_classes\n",
        "        self.n_samples = n_samples\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _ in range(10):  # Create 10 tasks\n",
        "            tasks_X, tasks_y = [], []\n",
        "            for class_id in range(self.n_classes):\n",
        "                class_samples = self.X.loc[self.y == class_id].sample(n=self.n_samples)\n",
        "                tasks_X.append(class_samples)\n",
        "                tasks_y.append(np.full((self.n_samples,), class_id))\n",
        "            yield pd.concat(tasks_X), np.concatenate(tasks_y)\n",
        "\n",
        "# Define the Model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(X_train.shape[1], 5)  # 5 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Training Loop with MAML\n",
        "model = SimpleModel()\n",
        "maml = MAML(model, lr=0.01, first_order=True)\n",
        "optimizer = Adam(maml.parameters(), lr=0.001)\n",
        "\n",
        "task_loader = TaskLoader(X_train, y_train)\n",
        "\n",
        "for task_X, task_y in task_loader:\n",
        "    learner = maml.clone()  # Create task-specific copy of the model\n",
        "\n",
        "    # Convert DataFrame to NumPy arrays and ensure they are numeric\n",
        "    task_X_values = task_X.values.astype(np.float32)  # Ensure float32\n",
        "    task_y_values = task_y.astype(np.int64)  # Ensure int64 for classification\n",
        "\n",
        "    # Create TensorDataset\n",
        "    task_dataset = TensorDataset(torch.tensor(task_X_values), torch.tensor(task_y_values))\n",
        "\n",
        "    task_loader = DataLoader(task_dataset, batch_size=2)\n",
        "\n",
        "    for batch_X, batch_y in task_loader:\n",
        "        pred = learner(batch_X)\n",
        "        loss = nn.CrossEntropyLoss()(pred, batch_y)\n",
        "        learner.adapt(loss)  # Adapt to the task\n",
        "\n",
        "    optimizer.step()  # Update the meta-learner\n",
        "\n",
        "    # Clear unnecessary variables and run garbage collection\n",
        "    del task_X, task_y, task_dataset\n",
        "    gc.collect()\n",
        "\n",
        "# After this point, you can evaluate the learner on unseen tasks or continue training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "\n",
        "# Load and Preprocess the Tabular Data\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\")\n",
        "data = data.dropna()\n",
        "X = data.drop(\"median_house_value\", axis=1)\n",
        "y = pd.qcut(data[\"median_house_value\"], q=5, labels=False)  # Binning target into classes\n",
        "X = pd.get_dummies(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the dataset class\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, tabular_data, image_dir, labels, transform=None):\n",
        "        self.tabular_data = tabular_data\n",
        "        self.image_dir = image_dir\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "        # Recursively collect all image files from the directory\n",
        "        self.image_files = []\n",
        "        for root, dirs, files in os.walk(image_dir):\n",
        "            for file in files:\n",
        "                if file.endswith(('.jpg', '.png', '.jpeg')):\n",
        "                    self.image_files.append(os.path.join(root, file))\n",
        "\n",
        "        # Check if images were found\n",
        "        if len(self.image_files) == 0:\n",
        "            raise ValueError(f\"No images found in directory: {image_dir}. Please check the path and file extensions.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.labels), len(self.image_files))  # Ensure the dataset length matches\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tabular_item = self.tabular_data.iloc[idx].values.astype(np.float32)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load an image by index\n",
        "        image_file = self.image_files[idx % len(self.image_files)]\n",
        "        image = Image.open(image_file).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return tabular_item, image, label\n",
        "\n",
        "# Define Transforms\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Update the path to your local dataset folder\n",
        "image_dataset_path = r\"C:\\Users\\Abhishek P\\Downloads\\archive\\seg_train\"\n",
        "\n",
        "# Print the directory contents to debug\n",
        "print(\"Files in dataset directory:\", os.listdir(image_dataset_path))\n",
        "\n",
        "train_dataset = MultiModalDataset(X_train, image_dataset_path, y_train.values, transform=image_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define Multi-Modal Model\n",
        "class MultiModalModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiModalModel, self).__init__()\n",
        "        self.image_model = models.resnet18(weights='DEFAULT')  # Use a pretrained ResNet\n",
        "        self.image_model.fc = nn.Linear(self.image_model.fc.in_features, 128)  # Adjust final layer\n",
        "        self.tabular_model = nn.Sequential(\n",
        "            nn.Linear(X_train.shape[1], 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32)\n",
        "        )\n",
        "        self.classifier = nn.Linear(128 + 32, 5)  # Combine features and classify into 5 classes\n",
        "\n",
        "    def forward(self, tabular_data, images):\n",
        "        image_features = self.image_model(images)\n",
        "        tabular_features = self.tabular_model(tabular_data)\n",
        "        combined_features = torch.cat((image_features, tabular_features), dim=1)\n",
        "        return self.classifier(combined_features)\n",
        "\n",
        "# Instantiate and Train the Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultiModalModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "model.train()\n",
        "for epoch in range(10):  # Change the number of epochs as needed\n",
        "    for tabular_data, images, labels in train_loader:\n",
        "        tabular_data, images, labels = tabular_data.to(device), images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(tabular_data, images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'multimodal_model.pth')\n",
        "\n",
        "# Evaluation can be added here to assess model performance on a test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files in dataset directory: ['seg_train']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Abhishek P\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Abhishek P\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 3.6423\n",
            "Epoch [2/5], Loss: 1.9112\n",
            "Epoch [3/5], Loss: 1.5659\n",
            "Epoch [4/5], Loss: 1.4868\n",
            "Epoch [5/5], Loss: 1.3757\n",
            "Average Test Loss: 2.0429\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "\n",
        "# Load and Preprocess the Tabular Data\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\")\n",
        "data = data.dropna()\n",
        "X = data.drop(\"median_house_value\", axis=1)\n",
        "y = pd.qcut(data[\"median_house_value\"], q=5, labels=False)  # Binning target into classes\n",
        "X = pd.get_dummies(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the dataset class\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, tabular_data, image_dir, labels, transform=None):\n",
        "        self.tabular_data = tabular_data\n",
        "        self.image_dir = image_dir\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "        # Recursively collect all image files from the directory\n",
        "        self.image_files = []\n",
        "        for root, dirs, files in os.walk(image_dir):\n",
        "            for file in files:\n",
        "                if file.endswith(('.jpg', '.png', '.jpeg')):\n",
        "                    self.image_files.append(os.path.join(root, file))\n",
        "\n",
        "        # Check if images were found\n",
        "        if len(self.image_files) == 0:\n",
        "            raise ValueError(f\"No images found in directory: {image_dir}. Please check the path and file extensions.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.labels), len(self.image_files))  # Ensure the dataset length matches\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tabular_item = self.tabular_data.iloc[idx].values.astype(np.float32)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load an image by index\n",
        "        image_file = self.image_files[idx % len(self.image_files)]\n",
        "        image = Image.open(image_file).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return tabular_item, image, label\n",
        "\n",
        "# Define Transforms\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Update the path to your local dataset folder\n",
        "image_dataset_path = r\"C:\\Users\\Abhishek P\\Downloads\\archive\\seg_train\"\n",
        "\n",
        "# Print the directory contents to debug\n",
        "print(\"Files in dataset directory:\", os.listdir(image_dataset_path))\n",
        "\n",
        "train_dataset = MultiModalDataset(X_train, image_dataset_path, y_train.values, transform=image_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Reduced batch size\n",
        "\n",
        "# Define Multi-Modal Model\n",
        "class MultiModalModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiModalModel, self).__init__()\n",
        "        self.image_model = models.resnet18(pretrained=True)  # Use a pretrained ResNet\n",
        "        self.image_model.fc = nn.Linear(self.image_model.fc.in_features, 128)  # Adjust final layer\n",
        "        self.tabular_model = nn.Sequential(\n",
        "            nn.Linear(X_train.shape[1], 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32)\n",
        "        )\n",
        "        self.classifier = nn.Linear(128 + 32, 5)  # Combine features and classify into 5 classes\n",
        "\n",
        "    def forward(self, tabular_data, images):\n",
        "        image_features = self.image_model(images)\n",
        "        tabular_features = self.tabular_model(tabular_data)\n",
        "        combined_features = torch.cat((image_features, tabular_features), dim=1)\n",
        "        return self.classifier(combined_features)\n",
        "\n",
        "# Instantiate the Model\n",
        "model = MultiModalModel()\n",
        "\n",
        "# Training Parameters\n",
        "device = torch.device(\"cpu\")  # Use CPU for training\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "model.train()\n",
        "epochs = 5  # Reduce epochs to 5 for faster training\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for tabular_data, images, labels in train_loader:\n",
        "        tabular_data, images, labels = tabular_data.to(device), images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(tabular_data, images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'multimodal_model.pth')\n",
        "\n",
        "# Evaluation on Test Set (Example)\n",
        "model.eval()\n",
        "test_losses = []\n",
        "for tabular_data, images, labels in train_loader:  # Example: using train_loader for demonstration\n",
        "    tabular_data, images, labels = tabular_data.to(device), images.to(device), labels.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tabular_data, images)\n",
        "        test_loss = criterion(outputs, labels)\n",
        "        test_losses.append(test_loss.item())\n",
        "\n",
        "average_test_loss = np.mean(test_losses)\n",
        "print(f\"Average Test Loss: {average_test_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchvision\n",
            "  Using cached torchvision-0.20.0-cp312-cp312-win_amd64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Collecting torch==2.5.0 (from torchvision)\n",
            "  Using cached torch-2.5.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\python312\\lib\\site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from torch==2.5.0->torchvision) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python312\\lib\\site-packages (from torch==2.5.0->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from torch==2.5.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from torch==2.5.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from torch==2.5.0->torchvision) (2024.6.0)\n",
            "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from torch==2.5.0->torchvision) (70.1.0)\n",
            "Collecting sympy==1.13.1 (from torch==2.5.0->torchvision)\n",
            "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python312\\lib\\site-packages (from sympy==1.13.1->torch==2.5.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch==2.5.0->torchvision) (2.1.5)\n",
            "Using cached torchvision-0.20.0-cp312-cp312-win_amd64.whl (1.6 MB)\n",
            "Using cached torch-2.5.0-cp312-cp312-win_amd64.whl (203.1 MB)\n",
            "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "Installing collected packages: sympy, torch, torchvision\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.12.1\n",
            "    Uninstalling sympy-1.12.1:\n",
            "      Successfully uninstalled sympy-1.12.1\n",
            "Successfully installed sympy-1.13.1 torch-2.5.0 torchvision-0.20.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.1 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "   !pip install torchvision --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in c:\\users\\abhishek p\\appdata\\roaming\\python\\python312\\site-packages (24.1)\n",
            "Collecting pip\n",
            "  Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Using cached pip-24.2-py3-none-any.whl (1.8 MB)\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1\n",
            "    Uninstalling pip-24.1:\n",
            "      Successfully uninstalled pip-24.1\n",
            "Successfully installed pip-24.2\n"
          ]
        }
      ],
      "source": [
        "   !python -m pip install --upgrade pip --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 29.58%\n"
          ]
        }
      ],
      "source": [
        "# Function to Evaluate Model Performance\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for tabular_data, images, labels in dataloader:\n",
        "            tabular_data, images, labels = tabular_data.to(device), images.to(device), labels.to(device)\n",
        "            outputs = model(tabular_data, images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "# Prepare Test Dataset and Dataloader\n",
        "test_dataset = MultiModalDataset(X_test, r\"C:\\Users\\Abhishek P\\Downloads\\archive\\seg_train\", y_test.values, transform=image_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Evaluate After Training\n",
        "accuracy = evaluate_model(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import deque\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Self-Improvement Class\n",
        "class SelfImprovement:\n",
        "    def __init__(self, model, learning_rate=0.01, discount_factor=0.95):\n",
        "        self.model = model\n",
        "        self.q_table = {}  # To store (state, action) => reward mappings\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if state in self.q_table and random.random() > 0.2:  # 80% chance of exploiting\n",
        "            return max(self.q_table[state], key=self.q_table[state].get)\n",
        "        else:\n",
        "            return random.choice(['adjust_lr', 'adjust_batch_size', 'retrain'])\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        old_value = self.q_table.get(state, {}).get(action, 0)\n",
        "        future_rewards = max(self.q_table.get(next_state, {}).values()) if next_state in self.q_table else 0\n",
        "        self.q_table.setdefault(state, {})[action] = old_value + self.learning_rate * (reward + self.discount_factor * future_rewards - old_value)\n",
        "\n",
        "    def feedback_loop(self, performance, history):\n",
        "        state = tuple(history[-2:])  # Use last two performances as state\n",
        "        reward = performance - history[-1]  # Reward is improvement over previous performance\n",
        "        action = self.select_action(state)\n",
        "        next_state = (history[-1], performance)\n",
        "\n",
        "        # Adjust model based on action\n",
        "        if action == 'adjust_lr':\n",
        "            for g in optimizer.param_groups:\n",
        "                g['lr'] = max(0.0001, g['lr'] * (1.1 if reward > 0 else 0.9))\n",
        "        elif action == 'adjust_batch_size':\n",
        "            train_loader.batch_size = min(64, train_loader.batch_size + (8 if reward > 0 else -8))\n",
        "        elif action == 'retrain':\n",
        "            model.train()  # Optional re-training step\n",
        "\n",
        "        self.update_q_table(state, action, reward, next_state)\n",
        "\n",
        "# Instantiate the Self-Improvement Module\n",
        "improvement = SelfImprovement(model)\n",
        "\n",
        "# Evaluate Model Performance Function\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for tabular_data, images, labels in dataloader:\n",
        "            tabular_data, images, labels = tabular_data.to(device), images.to(device), labels.to(device)\n",
        "            outputs = model(tabular_data, images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-18 22:20:40,558\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Current time: 2024-10-18 22:20:41 (running for 00:00:00.40)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 0/4 CPUs, 0/0 GPUs\n",
            "Result logdir: C:/Users/ABHISH~1/AppData/Local/Temp/ray/session_2024-10-18_22-01-19_729957_27424/artifacts/2024-10-18_22-20-40/tune_multimodal_model/driver_artifacts\n",
            "Number of trials: 10/10 (10 PENDING)\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2024-10-18 22:20:46 (running for 00:00:05.46)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 4.0/4 CPUs, 0/0 GPUs\n",
            "Result logdir: C:/Users/ABHISH~1/AppData/Local/Temp/ray/session_2024-10-18_22-01-19_729957_27424/artifacts/2024-10-18_22-20-40/tune_multimodal_model/driver_artifacts\n",
            "Number of trials: 10/10 (10 PENDING)\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2024-10-18 22:20:51 (running for 00:00:10.51)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 4.0/4 CPUs, 0/0 GPUs\n",
            "Result logdir: C:/Users/ABHISH~1/AppData/Local/Temp/ray/session_2024-10-18_22-01-19_729957_27424/artifacts/2024-10-18_22-20-40/tune_multimodal_model/driver_artifacts\n",
            "Number of trials: 10/10 (10 PENDING)\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2024-10-18 22:20:56 (running for 00:00:15.53)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 4.0/4 CPUs, 0/0 GPUs\n",
            "Result logdir: C:/Users/ABHISH~1/AppData/Local/Temp/ray/session_2024-10-18_22-01-19_729957_27424/artifacts/2024-10-18_22-20-40/tune_multimodal_model/driver_artifacts\n",
            "Number of trials: 10/10 (10 PENDING)\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2024-10-18 22:21:01 (running for 00:00:20.63)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 4.0/4 CPUs, 0/0 GPUs\n",
            "Result logdir: C:/Users/ABHISH~1/AppData/Local/Temp/ray/session_2024-10-18_22-01-19_729957_27424/artifacts/2024-10-18_22-20-40/tune_multimodal_model/driver_artifacts\n",
            "Number of trials: 10/10 (10 PENDING)\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2024-10-18 22:21:06 (running for 00:00:25.65)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 4.0/4 CPUs, 0/0 GPUs\n",
            "Result logdir: C:/Users/ABHISH~1/AppData/Local/Temp/ray/session_2024-10-18_22-01-19_729957_27424/artifacts/2024-10-18_22-20-40/tune_multimodal_model/driver_artifacts\n",
            "Number of trials: 10/10 (10 PENDING)\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-18 22:21:10,773\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
            "2024-10-18 22:21:12,936\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/Abhishek P/ray_results/tune_multimodal_model' in 2.1611s.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Current time: 2024-10-18 22:21:12 (running for 00:00:32.33)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 4.0/4 CPUs, 0/0 GPUs\n",
            "Result logdir: C:/Users/ABHISH~1/AppData/Local/Temp/ray/session_2024-10-18_22-01-19_729957_27424/artifacts/2024-10-18_22-20-40/tune_multimodal_model/driver_artifacts\n",
            "Number of trials: 10/10 (10 PENDING)\n",
            "+-------------------------+----------+-------+--------------+-----------------+\n",
            "| Trial name              | status   | loc   |   batch_size |   learning_rate |\n",
            "|-------------------------+----------+-------+--------------+-----------------|\n",
            "| train_model_1b9cb_00000 | PENDING  |       |           16 |     0.0274583   |\n",
            "| train_model_1b9cb_00001 | PENDING  |       |           64 |     0.0574695   |\n",
            "| train_model_1b9cb_00002 | PENDING  |       |           64 |     0.000311619 |\n",
            "| train_model_1b9cb_00003 | PENDING  |       |           16 |     0.000557732 |\n",
            "| train_model_1b9cb_00004 | PENDING  |       |           64 |     0.0134776   |\n",
            "| train_model_1b9cb_00005 | PENDING  |       |           64 |     0.0011664   |\n",
            "| train_model_1b9cb_00006 | PENDING  |       |           64 |     0.00862015  |\n",
            "| train_model_1b9cb_00007 | PENDING  |       |           32 |     0.0272845   |\n",
            "| train_model_1b9cb_00008 | PENDING  |       |           16 |     0.0015629   |\n",
            "| train_model_1b9cb_00009 | PENDING  |       |           16 |     0.000235802 |\n",
            "+-------------------------+----------+-------+--------------+-----------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-18 22:21:22,109\tINFO tune.py:1041 -- Total run time: 41.55 seconds (30.17 seconds for the tuning loop).\n",
            "2024-10-18 22:21:22,110\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
            "Resume experiment with: tune.run(..., resume=True)\n",
            "2024-10-18 22:21:22,130\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 10 trial(s):\n",
            "- train_model_1b9cb_00000: FileNotFoundError('Could not fetch metrics for train_model_1b9cb_00000: both result.json and progress.csv were not found at C:/Users/Abhishek P/ray_results/tune_multimodal_model/train_model_1b9cb_00000_0_batch_size=16,learning_rate=0.0275_2024-10-18_22-20-40')\n",
            "- train_model_1b9cb_00001: FileNotFoundError('Could not fetch metrics for train_model_1b9cb_00001: both result.json and progress.csv were not found at C:/Users/Abhishek P/ray_results/tune_multimodal_model/train_model_1b9cb_00001_1_batch_size=64,learning_rate=0.0575_2024-10-18_22-20-40')\n",
            "- train_model_1b9cb_00002: FileNotFoundError('Could not fetch metrics for train_model_1b9cb_00002: both result.json and progress.csv were not found at C:/Users/Abhishek P/ray_results/tune_multimodal_model/train_model_1b9cb_00002_2_batch_size=64,learning_rate=0.0003_2024-10-18_22-20-40')\n",
            "- train_model_1b9cb_00003: FileNotFoundError('Could not fetch metrics for train_model_1b9cb_00003: both result.json and progress.csv were not found at C:/Users/Abhishek P/ray_results/tune_multimodal_model/train_model_1b9cb_00003_3_batch_size=16,learning_rate=0.0006_2024-10-18_22-20-40')\n",
            "- train_model_1b9cb_00004: FileNotFoundError('Could not fetch metrics for train_model_1b9cb_00004: both result.json and progress.csv were not found at C:/Users/Abhishek P/ray_results/tune_multimodal_model/train_model_1b9cb_00004_4_batch_size=64,learning_rate=0.0135_2024-10-18_22-20-40')\n",
            "- train_model_1b9cb_00005: FileNotFoundError('Could not fetch metrics for train_model_1b9cb_00005: both result.json and progress.csv were not found at C:/Users/Abhishek P/ray_results/tune_multimodal_model/train_model_1b9cb_00005_5_batch_size=64,learning_rate=0.0012_2024-10-18_22-20-40')\n",
            "- train_model_1b9cb_00006: FileNotFoundError('Could not fetch metrics for train_model_1b9cb_00006: both result.json and progress.csv were not found at C:/Users/Abhishek P/ray_results/tune_multimodal_model/train_model_1b9cb_00006_6_batch_size=64,learning_rate=0.0086_2024-10-18_22-20-40')\n",
            "- train_model_1b9cb_00007: FileNotFoundError('Could not fetch metrics for train_model_1b9cb_00007: both result.json and progress.csv were not found at C:/Users/Abhishek P/ray_results/tune_multimodal_model/train_model_1b9cb_00007_7_batch_size=32,learning_rate=0.0273_2024-10-18_22-20-40')\n",
            "- train_model_1b9cb_00008: FileNotFoundError('Could not fetch metrics for train_model_1b9cb_00008: both result.json and progress.csv were not found at C:/Users/Abhishek P/ray_results/tune_multimodal_model/train_model_1b9cb_00008_8_batch_size=16,learning_rate=0.0016_2024-10-18_22-20-40')\n",
            "- train_model_1b9cb_00009: FileNotFoundError('Could not fetch metrics for train_model_1b9cb_00009: both result.json and progress.csv were not found at C:/Users/Abhishek P/ray_results/tune_multimodal_model/train_model_1b9cb_00009_9_batch_size=16,learning_rate=0.0002_2024-10-18_22-20-40')\n",
            "2024-10-18 22:21:22,131\tWARNING experiment_analysis.py:558 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'config'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 154\u001b[0m\n\u001b[0;32m    151\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(compressed_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_compressed_multimodal_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[43mtune_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[20], line 139\u001b[0m, in \u001b[0;36mtune_hyperparameters\u001b[1;34m(num_samples, num_epochs)\u001b[0m\n\u001b[0;32m    128\u001b[0m result \u001b[38;5;241m=\u001b[39m tune\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    129\u001b[0m     tune\u001b[38;5;241m.\u001b[39mwith_parameters(train_model, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs),\n\u001b[0;32m    130\u001b[0m     resources_per_trial\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m},  \u001b[38;5;66;03m# Reduce CPU requirement and remove GPU requirement\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtune_multimodal_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m )\n\u001b[0;32m    138\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget_best_trial(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbest_trial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial final validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_trial\u001b[38;5;241m.\u001b[39mlast_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Load the best model\u001b[39;00m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'config'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, DistributedSampler\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.strategies import DDPStrategy\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ray import tune\n",
        "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
        "\n",
        "class MultiModalDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tabular_data, image_dir, labels, transform=None):\n",
        "        self.tabular_data = tabular_data\n",
        "        self.image_dir = image_dir\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.labels), len(self.image_files))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tabular_item = torch.tensor(self.tabular_data.iloc[idx].values, dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        image_file = os.path.join(self.image_dir, self.image_files[idx % len(self.image_files)])\n",
        "        image = Image.open(image_file).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return tabular_item, image, label\n",
        "\n",
        "class MultiModalModel(LightningModule):\n",
        "    def __init__(self, tabular_features, learning_rate=0.001):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.image_model = models.resnet18(weights='DEFAULT')\n",
        "        self.image_model.fc = nn.Linear(self.image_model.fc.in_features, 128)\n",
        "        self.tabular_model = nn.Sequential(\n",
        "            nn.Linear(tabular_features, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32)\n",
        "        )\n",
        "        self.classifier = nn.Linear(128 + 32, 5)  # 5 classes for binned house values\n",
        "\n",
        "    def forward(self, tabular_data, images):\n",
        "        image_features = self.image_model(images)\n",
        "        tabular_features = self.tabular_model(tabular_data)\n",
        "        combined_features = torch.cat((image_features, tabular_features), dim=1)\n",
        "        return self.classifier(combined_features)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        tabular_data, images, labels = batch\n",
        "        outputs = self(tabular_data, images)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, sync_dist=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        tabular_data, images, labels = batch\n",
        "        outputs = self(tabular_data, images)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        self.log('val_loss', loss, on_step=True, on_epoch=True, sync_dist=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
        "\n",
        "def train_model(config, num_epochs=10):\n",
        "    # Load and preprocess the data\n",
        "    tabular_data = pd.read_csv(\"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\")\n",
        "    labels = pd.cut(tabular_data.pop('median_house_value'), bins=5, labels=False)\n",
        "    image_dataset_path = r\"C:\\Users\\Abhishek P\\Downloads\\archive\\seg_train\"  # Update with your actual path\n",
        "\n",
        "    # Setup dataset\n",
        "    image_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    dataset = MultiModalDataset(tabular_data, image_dataset_path, labels, transform=image_transform)\n",
        "    train_set, val_set = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=config[\"batch_size\"], num_workers=4, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=config[\"batch_size\"], num_workers=4)\n",
        "\n",
        "   # Initialize model\n",
        "    model = MultiModalModel(tabular_features=tabular_data.shape[1], learning_rate=config[\"learning_rate\"])\n",
        "\n",
        "    # Setup callbacks\n",
        "    checkpoint_callback = ModelCheckpoint(monitor='val_loss')\n",
        "    tune_report_callback = TuneReportCallback({\"val_loss\": \"val_loss\"}, on=\"validation_end\")\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        max_epochs=num_epochs,\n",
        "        accelerator=\"auto\",\n",
        "        devices=\"auto\",\n",
        "        strategy=DDPStrategy(find_unused_parameters=False),\n",
        "        callbacks=[checkpoint_callback, tune_report_callback],\n",
        "        logger=TensorBoardLogger(save_dir=tune.get_trial_dir(), name=\"\", version=\".\"),\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "def tune_hyperparameters(num_samples=10, num_epochs=10):\n",
        "    config = {\n",
        "        \"batch_size\": tune.choice([16, 32, 64]),\n",
        "        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
        "    }\n",
        "\n",
        "    scheduler = tune.schedulers.ASHAScheduler(\n",
        "        max_t=num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2,\n",
        "        metric=\"val_loss\",  # Specify the metric to optimize\n",
        "        mode=\"min\"  # We want to minimize the validation loss\n",
        "    )\n",
        "    reporter = tune.CLIReporter(\n",
        "        parameter_columns=[\"batch_size\", \"learning_rate\"],\n",
        "        metric_columns=[\"val_loss\", \"training_iteration\"])\n",
        "\n",
        "    result = tune.run(\n",
        "        tune.with_parameters(train_model, num_epochs=num_epochs),\n",
        "        resources_per_trial={\"cpu\": 1, \"gpu\": 0},  # Reduce CPU requirement and remove GPU requirement\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter,\n",
        "        name=\"tune_multimodal_model\"\n",
        "    )\n",
        "\n",
        "    best_trial = result.get_best_trial(\"val_loss\", \"min\", \"last\")\n",
        "    print(f\"Best trial config: {best_trial.config}\")\n",
        "    print(f\"Best trial final validation loss: {best_trial.last_result['val_loss']}\")\n",
        "\n",
        "    # Load the best model\n",
        "    best_model = MultiModalModel.load_from_checkpoint(best_trial.checkpoint.value)\n",
        "    \n",
        "    # Compress the model\n",
        "    compressed_model = torch.quantization.quantize_dynamic(\n",
        "        best_model, {nn.Linear}, dtype=torch.qint8\n",
        "    )\n",
        "    \n",
        "    # Save the compressed model\n",
        "    torch.save(compressed_model.state_dict(), 'best_compressed_multimodal_model.pth')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tune_hyperparameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Abhishek P\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Abhishek P\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name          | Type       | Params | Mode \n",
            "-----------------------------------------------------\n",
            "0 | image_model   | ResNet     | 11.2 M | train\n",
            "1 | tabular_model | Sequential | 2.7 K  | train\n",
            "2 | classifier    | Linear     | 805    | train\n",
            "-----------------------------------------------------\n",
            "11.2 M    Trainable params\n",
            "0         Non-trainable params\n",
            "11.2 M    Total params\n",
            "44.983    Total estimated model params size (MB)\n",
            "73        Modules in train mode\n",
            "0         Modules in eval mode\n",
            "C:\\Users\\Abhishek P\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
            "C:\\Users\\Abhishek P\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_lightning\\utilities\\data.py:105: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
            "C:\\Users\\Abhishek P\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_lightning\\utilities\\data.py:105: Total length of `CombinedLoader` across ranks is zero. Please make sure this was your intention.\n",
            "`Trainer.fit` stopped: No training batches.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.distributed import DistributedSampler  # Corrected import\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess the data\n",
        "tabular_data = pd.read_csv(\"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\")\n",
        "labels = tabular_data.pop('median_house_value').values  # Use 'median_house_value' as the target\n",
        "image_dataset_path = r\"C:\\Users\\Abhishek P\\Downloads\\archive\\seg_train\"  # Update with your actual path\n",
        "\n",
        "# Bin the labels into 5 classes\n",
        "labels_binned = pd.cut(labels, bins=5, labels=False)\n",
        "\n",
        "class MultiModalDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tabular_data, image_dir, labels, transform=None):\n",
        "        self.tabular_data = tabular_data\n",
        "        self.image_dir = image_dir\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.labels), len(self.image_files))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tabular_item = torch.tensor(self.tabular_data.iloc[idx].values, dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        image_file = os.path.join(self.image_dir, self.image_files[idx % len(self.image_files)])\n",
        "        image = Image.open(image_file).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return tabular_item, image, label\n",
        "\n",
        "class MultiModalModel(LightningModule):\n",
        "    def __init__(self, tabular_features):\n",
        "        super().__init__()\n",
        "        self.image_model = models.resnet18(pretrained=True)\n",
        "        self.image_model.fc = nn.Linear(self.image_model.fc.in_features, 128)\n",
        "        self.tabular_model = nn.Sequential(\n",
        "            nn.Linear(tabular_features, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32)\n",
        "        )\n",
        "        self.classifier = nn.Linear(128 + 32, 5)  # 5 classes for binned house values\n",
        "\n",
        "    def forward(self, tabular_data, images):\n",
        "        image_features = self.image_model(images)\n",
        "        tabular_features = self.tabular_model(tabular_data)\n",
        "        combined_features = torch.cat((image_features, tabular_features), dim=1)\n",
        "        return self.classifier(combined_features)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        tabular_data, images, labels = batch\n",
        "        outputs = self(tabular_data, images)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
        "\n",
        "def main():\n",
        "    # Setup dataset\n",
        "    image_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    dataset = MultiModalDataset(tabular_data, image_dataset_path, labels_binned, transform=image_transform)\n",
        "    train_loader = DataLoader(dataset, batch_size=32, num_workers=0)  # Set num_workers to 0 for debugging\n",
        "\n",
        "    # Initialize model\n",
        "    model = MultiModalModel(tabular_features=tabular_data.shape[1])\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        max_epochs=5,\n",
        "        accelerator=\"auto\",\n",
        "        devices=1,  # Use a single device\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.fit(model, train_loader)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.12 (Scripts)",
      "language": "python",
      "name": "scripts"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "37c1e46173e24f7696e0eb6d2e018b4d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "3f030dfc4ef945debee74388acf789b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6a1a6bc753d4d89a40b5852435d8347",
            "placeholder": "​",
            "style": "IPY_MODEL_f11e0809417d4f07bce601b0b3c32087",
            "value": "Optimization Progress: "
          }
        },
        "5158cd731c5647c89f2005724128e926": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e742c431b17c4444a604d9abaee9f76f",
            "placeholder": "​",
            "style": "IPY_MODEL_c45e038f8ece4538b8e7fc114fe93e17",
            "value": " 301/? [1:11:45&lt;00:00, 29.75s/pipeline]"
          }
        },
        "7519b1a6f60a433db213314a972295c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bcfbce17c4c945f0a736edba9fa1e553": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c45e038f8ece4538b8e7fc114fe93e17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e469ca1328ab4199a6703ca6bce76310": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f030dfc4ef945debee74388acf789b4",
              "IPY_MODEL_fe4da3b043e4467d82d06ba0d994e9ba",
              "IPY_MODEL_5158cd731c5647c89f2005724128e926"
            ],
            "layout": "IPY_MODEL_37c1e46173e24f7696e0eb6d2e018b4d"
          }
        },
        "e6a1a6bc753d4d89a40b5852435d8347": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e742c431b17c4444a604d9abaee9f76f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f11e0809417d4f07bce601b0b3c32087": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe4da3b043e4467d82d06ba0d994e9ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcfbce17c4c945f0a736edba9fa1e553",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7519b1a6f60a433db213314a972295c7",
            "value": 300
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
